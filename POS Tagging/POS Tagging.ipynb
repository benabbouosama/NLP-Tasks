{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging:"
      ],
      "metadata": {
        "id": "14doanpmwo10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a POS (Part-of-Speech) tagging system for the French language using the data indicated in the following link: [ANTILLES Data Repository](https://github.com/qanastek/ANTILLES/tree/main/ANTILLES)."
      ],
      "metadata": {
        "id": "r4sBBwBwJImi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "dpvvPUkeJ_31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vULeyvDeHs_D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_words_and_pos_tags(input_file_path, output_file_path):\n",
        "    words_pos_tags = []\n",
        "\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            # Check if the line marks the start of a new sentence\n",
        "            if line.startswith(\"# text = \"):\n",
        "                words_pos_tags.append((\"<s>\", \"\"))  # Add sentence start marker <s>\n",
        "                continue  # Move to the next line\n",
        "\n",
        "            # Skip comment lines\n",
        "            if line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            # Process the line if it contains valid word and POS tag information\n",
        "            fields = line.strip().split(\"\\t\")\n",
        "\n",
        "            # Ensure that the line contains the necessary fields (at least 4)\n",
        "            if len(fields) > 3:\n",
        "                word = fields[1]  # Word is in the second field\n",
        "                pos_tag = fields[3]  # POS tag is in the fourth field\n",
        "                words_pos_tags.append((word, pos_tag))\n",
        "\n",
        "    # Write the extracted words and POS tags to the output file\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "        output_file.writelines(f\"{word}\\t{pos_tag}\\n\" for word, pos_tag in words_pos_tags)"
      ],
      "metadata": {
        "id": "W0WwoKgCwqb9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_words_and_pos_tags(\"train.conllu\",\"preprocessed_train.txt\")\n",
        "extract_words_and_pos_tags(\"dev.conllu\",\"preprocessed_dev.txt\")"
      ],
      "metadata": {
        "id": "6YZRZkziBrMm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dev(input_file_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    pos_tags_sentences = []\n",
        "    pos_tag_sentence = []\n",
        "\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            # Skip comment lines starting with '#'\n",
        "            if not line.startswith(\"#\"):\n",
        "                # Split the line by tabs to extract fields\n",
        "                fields = line.strip().split(\"\\t\")\n",
        "\n",
        "                # Ensure the line contains at least two fields (word and POS tag)\n",
        "                if len(fields) > 1:\n",
        "                    word = fields[0]  # Word is the first field\n",
        "                    pos_tag = fields[1]  # POS tag is the second field\n",
        "                    sentence.append(word)\n",
        "                    pos_tag_sentence.append(pos_tag)\n",
        "                else:\n",
        "                    # If it's an empty line, it marks the end of a sentence\n",
        "                    if sentence and pos_tag_sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        pos_tags_sentences.append(pos_tag_sentence)\n",
        "                    # Reset sentence and POS tag list for the next sentence\n",
        "                    sentence = []\n",
        "                    pos_tag_sentence = []\n",
        "\n",
        "    # Ensure to remove any empty sentence at the start if present\n",
        "    if sentences and pos_tags_sentences:\n",
        "        sentences.pop(0)\n",
        "        pos_tags_sentences.pop(0)\n",
        "\n",
        "    # Return the list of sentences and POS tags\n",
        "    return sentences, pos_tags_sentences"
      ],
      "metadata": {
        "id": "kOe63l7RB68N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_words, dev_pos_tags = prepare_dev(\"preprocessed_dev.txt\")"
      ],
      "metadata": {
        "id": "WUdGMbSyCPUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Counts"
      ],
      "metadata": {
        "id": "pY-h2mPNCtz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **emission_counts**: Stores the number of times a word is tagged by a specific tag.\n",
        "\n",
        "- **transition_counts**: Stores the number of times a tag is preceded by another tag.\n",
        "\n",
        "- **tag_counts**: Stores the number of occurrences of a tag.\n",
        "\n",
        "- **word_counts**: Stores the number of occurrences of a word."
      ],
      "metadata": {
        "id": "63FN4CT0KZjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Unknowns\n",
        "\n",
        "This section describes the process of managing unknown words in the dataset. Unknown words are words that do not appear in the training data and need to be categorized appropriately for model training and evaluation. The following strategies can be used to handle unknowns:\n",
        "\n",
        "1. **Assigning Unknown Tags**: Use a function to assign specific tags to unknown words based on their characteristics, such as:\n",
        "   - **Digits**: Tag words containing numbers as `--unk_digit--`.\n",
        "   - **Punctuation**: Tag words containing punctuation as `--unk_punct--`.\n",
        "   - **Uppercase Words**: Tag words starting with an uppercase letter as `--unk_upper--`.\n",
        "   - **Suffix Analysis**: Classify unknown words as nouns, verbs, adjectives, or adverbs based on common suffixes.\n",
        "\n",
        "2. **Updating Word Counts**: When processing the corpus, replace unknown words with their assigned tags to ensure that the counts reflect the presence of these words in the data.\n",
        "\n",
        "3. **Impact on Model Training**: Ensure that the model is trained to recognize and handle unknown words effectively, improving its robustness and accuracy in real-world applications.\n",
        "\n",
        "By implementing these strategies, the system can manage unknown words more effectively and maintain the integrity of the tagging process.\n"
      ],
      "metadata": {
        "id": "qPMc2Fy2DLXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_unk(tok):\n",
        "    punct = set(string.punctuation)\n",
        "\n",
        "    # Common French noun suffixes\n",
        "    noun_suffix = [\n",
        "        \"ance\", \"ence\", \"it√©\", \"isme\", \"ment\", \"eur\", \"ion\", \"ure\", \"ade\", \"age\", \"oire\",\n",
        "        \"esse\", \"ette\", \"ance\", \"ie\", \"ance\", \"ence\", \"erie\", \"ence\"\n",
        "    ]\n",
        "\n",
        "    # Common French verb suffixes\n",
        "    verb_suffix = [\"er\", \"ir\", \"re\", \"oir\", \"ifier\", \"iser\", \"ayer\"]\n",
        "\n",
        "    # Common French adjective suffixes\n",
        "    adj_suffix = [\n",
        "        \"able\", \"ible\", \"ant\", \"ent\", \"el\", \"al\", \"if\", \"ive\", \"eux\", \"euse\", \"ais\",\n",
        "        \"ien\", \"ienne\", \"ique\", \"in\", \"ine\", \"on\", \"onne\"\n",
        "    ]\n",
        "\n",
        "    # Common French adverb suffixes\n",
        "    adv_suffix = [\"ment\", \"amment\", \"emment\"]\n",
        "\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case (e.g., proper nouns or sentence start)\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    # Default unknown token\n",
        "    return \"--unk--\""
      ],
      "metadata": {
        "id": "5xlgZJrRCrsJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_unknowns(word_counts, threshold):\n",
        "    # Dictionary to store unknown word mappings\n",
        "    unknowns = {}\n",
        "\n",
        "    # Dictionary to store updated word counts, initialized with int\n",
        "    new_word_count = defaultdict(int)\n",
        "\n",
        "    # Loop through each word and its count in the word_counts dictionary\n",
        "    for word, count in word_counts.items():\n",
        "        # If the word count is less than the threshold, classify it as unknown\n",
        "        if count < threshold:\n",
        "            unk_label = assign_unk(word)  # Get the unknown label for the word\n",
        "            unknowns[word] = unk_label  # Map the word to its unknown label\n",
        "            new_word_count[unk_label] += count  # Increment the count for the unknown category\n",
        "        else:\n",
        "            # Keep the word if it meets the threshold\n",
        "            new_word_count[word] = count\n",
        "\n",
        "    return unknowns, new_word_count"
      ],
      "metadata": {
        "id": "MLHvWI0OC-9P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize defaultdicts for counting emissions, transitions, tags, and words\n",
        "emission_counts = defaultdict(int)\n",
        "transition_counts = defaultdict(int)\n",
        "tag_counts = defaultdict(int)\n",
        "word_counts = defaultdict(int)\n",
        "\n",
        "# Read the file and process the lines\n",
        "with open(\"preprocessed_train.txt\", 'r', encoding=\"utf-8\") as file:\n",
        "    tags = []  # List to keep track of tags in sequence\n",
        "    for line in file:\n",
        "        columns = line.strip().split(\"\\t\")  # Split the line by tabs\n",
        "\n",
        "        # Handle sentence start tag\n",
        "        if columns[0] == \"<s>\":\n",
        "            tags = [\"<s>\"]  # Reset tags to start of sentence\n",
        "            tag_counts[\"<s>\"] += 1  # Increment the count of start-of-sentence tags\n",
        "        else:\n",
        "            # Add the current tag (last column) to the tags list\n",
        "            tags.append(columns[-1])\n",
        "\n",
        "        # Handle tag transitions\n",
        "        if len(tags) == 2:\n",
        "            transition_counts[tuple(tags)] += 1  # Record the tag transition\n",
        "            tags.pop(0)  # Keep only the most recent tag\n",
        "\n",
        "        # Process words and their associated tags if the line contains both\n",
        "        if len(columns) == 2:\n",
        "            word, tag = columns  # Extract word and tag from columns\n",
        "            emission_counts[(tag, word)] += 1  # Record the tag-word emission\n",
        "            tag_counts[tag] += 1  # Increment the tag count\n",
        "            word_counts[word] += 1  # Increment the word count"
      ],
      "metadata": {
        "id": "q59kNSfeDHIg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unknowns,word_counts = define_unknowns(word_counts,5)"
      ],
      "metadata": {
        "id": "tug5FHoSEBbX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that replaces unknown words with their new tag"
      ],
      "metadata": {
        "id": "-kpQpihiEK3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_emission(emission_counts, unknowns):\n",
        "    # Initialize a defaultdict to store the new emission counts\n",
        "    new_emission_count = defaultdict(int)\n",
        "\n",
        "    # Iterate through each emission and its count\n",
        "    for (tag, word), count in emission_counts.items():\n",
        "        # Check if the word is among the unknowns\n",
        "        if word in unknowns:\n",
        "            # Create a new emission with the unknown tag\n",
        "            new_emission = (tag, unknowns[word])\n",
        "            new_emission_count[new_emission] += count  # Update the count for the new emission\n",
        "        else:\n",
        "            # Retain the original emission count if the word is known\n",
        "            new_emission_count[(tag, word)] = count\n",
        "\n",
        "    return new_emission_count"
      ],
      "metadata": {
        "id": "flGRPGUmELfN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emission_counts = change_emission(emission_counts,unknowns)"
      ],
      "metadata": {
        "id": "8AesEwr9ERhL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transition Matrix A\n",
        "\n",
        "This matrix stores the probabilities of all possible transitions between tags."
      ],
      "metadata": {
        "id": "8FeOBE0FEUze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transition_matrix(transition_counts, tag_counts, smoothing_coefficient):\n",
        "    \"\"\"\n",
        "    Computes the transition matrix A for the given tag counts and transition counts.\n",
        "\n",
        "    Parameters:\n",
        "    - transition_counts: A dictionary containing counts of transitions (tag_from, tag_to) pairs.\n",
        "    - tag_counts: A dictionary containing counts of each tag.\n",
        "    - smoothing_coefficient: A float value used for smoothing to handle zero probabilities.\n",
        "\n",
        "    Returns:\n",
        "    - A: A 2D numpy array representing the transition probabilities between tags.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the number of unique tags\n",
        "    num_tags = len(tag_counts)\n",
        "\n",
        "    # Sort the tags to maintain a consistent order\n",
        "    tags = sorted(tag_counts.keys())\n",
        "\n",
        "    # Initialize a transition matrix A with zeros\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    # Calculate transition probabilities with smoothing\n",
        "    for i, tag_from in enumerate(tags):\n",
        "        for j, tag_to in enumerate(tags):\n",
        "            tag_pair = (tag_from, tag_to)\n",
        "            tag_pair_count = transition_counts.get(tag_pair, 0)  # Default to 0 if not found\n",
        "\n",
        "            # Apply smoothing to calculate the probability\n",
        "            tag_pair_probability = (tag_pair_count + smoothing_coefficient) / (tag_counts[tag_from] + smoothing_coefficient * num_tags)\n",
        "            A[i, j] = tag_pair_probability\n",
        "\n",
        "    return A"
      ],
      "metadata": {
        "id": "R5VU57pLEcUw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = transition_matrix(transition_counts, tag_counts, 0.02)"
      ],
      "metadata": {
        "id": "mu7FFYEhEpwi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emission Matrix B\n",
        "\n",
        "This matrix stores the probabilities of all possible emissions between tags and words."
      ],
      "metadata": {
        "id": "gZ6oCAwLFCTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emission_matrix(emission_counts, tag_counts, word_counts, smoothing_coefficient):\n",
        "    \"\"\"\n",
        "    Computes the emission matrix B.\n",
        "\n",
        "    Parameters:\n",
        "    - emission_counts: A dictionary containing counts of emissions (tag, word) pairs.\n",
        "    - tag_counts: A dictionary containing counts of each tag.\n",
        "    - word_counts: A dictionary containing counts of each word.\n",
        "    - smoothing_coefficient: A float value used for smoothing.\n",
        "\n",
        "    Returns:\n",
        "    - B: A 2D numpy array representing the emission probabilities between tags and words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get sorted lists of tags and words\n",
        "    tags = sorted(tag_counts.keys())\n",
        "    words = sorted(word_counts.keys())\n",
        "\n",
        "    # Initialize the emission matrix with zeros\n",
        "    num_tags = len(tags)\n",
        "    num_words = len(words)\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    # Calculate the emission probabilities for each tag-word pair\n",
        "    for i, tag in enumerate(tags):\n",
        "        for j, word in enumerate(words):\n",
        "            tag_word_pair = (tag, word)\n",
        "            emission_count = emission_counts.get(tag_word_pair, 0)  # Use 0 if the pair is not found\n",
        "            # Calculate probability with smoothing\n",
        "            tag_probability = (emission_count + smoothing_coefficient) / (tag_counts[tag] + smoothing_coefficient * num_words)\n",
        "            B[i, j] = tag_probability\n",
        "\n",
        "    return B"
      ],
      "metadata": {
        "id": "q9IkjvO0E8jl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = emission_matrix(emission_counts, tag_counts,word_counts, 0.02)"
      ],
      "metadata": {
        "id": "SnquKr2BFXVz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viterbi Algorithm\n",
        "\n",
        "Using the two matrices A and B, we will populate the two matrices used in the Viterbi algorithm:\n",
        "- Matrix C, which stores the probabilities of transitioning from a POS tag to a word.\n",
        "- Matrix D, which will keep track of the best path taken."
      ],
      "metadata": {
        "id": "rYESWO99FmsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViterbiAlgorithm:\n",
        "    def __init__(self, A, B, tag_counts, word_counts):\n",
        "        \"\"\"\n",
        "        Initialize the ViterbiAlgorithm with transition matrix A, emission matrix B,\n",
        "        tag counts, and word counts.\n",
        "\n",
        "        Args:\n",
        "            A (np.ndarray): Transition matrix.\n",
        "            B (np.ndarray): Emission matrix.\n",
        "            tag_counts (dict): Dictionary of tag counts.\n",
        "            word_counts (dict): Dictionary of word counts.\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.tag_counts = tag_counts\n",
        "        self.word_counts = word_counts\n",
        "        self.tags = sorted(tag_counts.keys())\n",
        "        self.number_tags = len(self.tags)\n",
        "\n",
        "    def get_index_of_key(self, dictionary, key):\n",
        "        \"\"\"\n",
        "        Get the index of a key in a sorted dictionary.\n",
        "\n",
        "        Args:\n",
        "            dictionary (list): The list of dictionary keys.\n",
        "            key: The key to search for.\n",
        "\n",
        "        Returns:\n",
        "            int: The index of the key, or None if not found.\n",
        "        \"\"\"\n",
        "        for index, dict_key in enumerate(dictionary):\n",
        "            if dict_key == key:\n",
        "                return index\n",
        "        return None\n",
        "\n",
        "    def initialisation(self, corpus):\n",
        "        \"\"\"\n",
        "        Initialize matrices C and D and preprocess the corpus.\n",
        "\n",
        "        Args:\n",
        "            corpus (list): The input corpus (list of tokens).\n",
        "\n",
        "        Returns:\n",
        "            tuple: Matrices C, D, and the new corpus with unknown tokens replaced.\n",
        "        \"\"\"\n",
        "        new_corpus = [token if token in self.word_counts else assign_unk(token) for token in corpus]\n",
        "\n",
        "        C = np.zeros((self.number_tags, len(new_corpus)))\n",
        "        D = np.zeros((self.number_tags, len(new_corpus)))\n",
        "\n",
        "        index_of_first_word = self.get_index_of_key(sorted(self.word_counts.keys()), new_corpus[0])\n",
        "        for i in range(self.number_tags):\n",
        "            C[i, 0] = self.A[0, i] * self.B[i, index_of_first_word]\n",
        "\n",
        "        return C, D, new_corpus\n",
        "\n",
        "    def forward_pass(self, C, D, new_corpus):\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the Viterbi algorithm.\n",
        "\n",
        "        Args:\n",
        "            C (np.ndarray): Matrix C for probabilities.\n",
        "            D (np.ndarray): Matrix D for storing best paths.\n",
        "            new_corpus (list): The processed corpus.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Updated matrices C and D.\n",
        "        \"\"\"\n",
        "        for j in range(1, len(new_corpus)):\n",
        "            for i in range(self.number_tags):\n",
        "                max_value = 0\n",
        "                max_index = 0\n",
        "                index_of_word = self.get_index_of_key(sorted(self.word_counts.keys()), new_corpus[j])\n",
        "\n",
        "                for i_tag in range(self.number_tags):\n",
        "                    value = C[i_tag, j - 1] * self.A[i_tag, i] * self.B[i, index_of_word]\n",
        "                    if value > max_value:\n",
        "                        max_value = value\n",
        "                        max_index = i_tag\n",
        "\n",
        "                C[i, j] = max_value\n",
        "                D[i, j] = max_index\n",
        "\n",
        "        return C, D\n",
        "\n",
        "    def backward_pass(self, C, D, new_corpus):\n",
        "        \"\"\"\n",
        "        Perform the backward pass of the Viterbi algorithm to retrieve the predicted tags.\n",
        "\n",
        "        Args:\n",
        "            C (np.ndarray): Matrix C with probabilities.\n",
        "            D (np.ndarray): Matrix D with best paths.\n",
        "            new_corpus (list): The processed corpus.\n",
        "\n",
        "        Returns:\n",
        "            list: The predicted tags for the corpus.\n",
        "        \"\"\"\n",
        "        corpus_length = len(new_corpus)\n",
        "        best_proba = 0\n",
        "        predicted_last_tags_indexes = 0\n",
        "        predicted_tags = [None] * corpus_length\n",
        "\n",
        "        for i in range(self.number_tags):\n",
        "            if C[i, -1] > best_proba:\n",
        "                best_proba = C[i, -1]\n",
        "                predicted_last_tags_indexes = i\n",
        "\n",
        "        predicted_tags[corpus_length - 1] = self.tags[predicted_last_tags_indexes]\n",
        "\n",
        "        for j in range(corpus_length - 2, -1, -1):\n",
        "            pos_tag_j = D[np.argmax(C[:, j + 1]), j + 1]\n",
        "            predicted_tags[j] = self.tags[int(pos_tag_j)]\n",
        "\n",
        "        return predicted_tags\n",
        "\n",
        "    def compute_accuracy(self, pred, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the predicted tags against the true tags.\n",
        "\n",
        "        Args:\n",
        "            pred (list): List of predicted tags.\n",
        "            y (list): List of true tags.\n",
        "\n",
        "        Returns:\n",
        "            float: The accuracy as a fraction of correct predictions.\n",
        "        \"\"\"\n",
        "        num_correct = sum(p == t for p, t in zip(pred, y))\n",
        "        total = len(pred)\n",
        "        return num_correct / total if total > 0 else 0\n",
        "\n",
        "    def run(self, validation_words, validation_pos_tags):\n",
        "        \"\"\"\n",
        "        Run the Viterbi algorithm and print results.\n",
        "\n",
        "        Args:\n",
        "            validation_words (list): List of words for validation.\n",
        "            validation_pos_tags (list): List of original POS tags for validation.\n",
        "        \"\"\"\n",
        "        C, D, new_corpus = self.initialisation(validation_words)\n",
        "        C, D = self.forward_pass(C, D, new_corpus)\n",
        "        predicted_pos_tags = self.backward_pass(C, D, new_corpus)\n",
        "\n",
        "        print(\"Generated tags by the algorithm:\")\n",
        "        print(predicted_pos_tags)\n",
        "        print(\"Original tags:\")\n",
        "        print(validation_pos_tags)\n",
        "        print(\"Accuracy:\")\n",
        "        print(self.compute_accuracy(predicted_pos_tags, validation_pos_tags))\n",
        "\n"
      ],
      "metadata": {
        "id": "R5v0JswsFat7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi = ViterbiAlgorithm(A, B, tag_counts, word_counts)\n",
        "viterbi.run(dev_words[15], dev_pos_tags[15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8C8PouEGa27",
        "outputId": "48557d1f-a56b-4c93-9b36-e90b7662fead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated tags by the algorithm:\n",
            "['PROPN', 'CHIF', 'NMP', 'PREL', 'DINTMS', 'NMS', 'ADJMS', 'PREP', 'CHIF', 'NMP', 'COCO', 'DINTFS', 'NFS', 'PREP', 'NFS', 'PUNCT', 'PDEMMS', 'AUX', 'DINTFS', 'NFS', 'ADJFS', 'PREP', 'NFS', 'YPFOR']\n",
            "Original tags:\n",
            "['VERB', 'CHIF', 'NMP', 'PREL', 'DINTMS', 'NMS', 'VERB', 'PREP', 'CHIF', 'NMP', 'COCO', 'DINTFS', 'NFS', 'PREP', 'NFS', 'PUNCT', 'PDEMMS', 'AUX', 'DINTFS', 'NFS', 'ADJFS', 'PREP', 'NFS', 'YPFOR']\n",
            "Accuracy:\n",
            "0.9166666666666666\n"
          ]
        }
      ]
    }
  ]
}