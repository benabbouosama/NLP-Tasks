{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram Language Models"
      ],
      "metadata": {
        "id": "kj9utRMKgCPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main objective of this notebook is to implement an n-gram language model and evaluate its performance. This model will subsequently be applied to text generation, automatic correction, and auto-completion."
      ],
      "metadata": {
        "id": "fZmijJ_GgCX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4yMMs0dgHhQ",
        "outputId": "5a5eeebf-f110-4ac4-a220-5d4e13645752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method: `prepare_data(infile, operation)`\n",
        "\n",
        "This method processes a text file as input, representing a corpus, and performs several preprocessing steps. It tokenizes the text into words, normalizes the text to lowercase, and adds start (`<s>`) and end (`</s>`) tokens to each sentence for N-gram modeling. Additionally, it handles out-of-vocabulary words by identifying those that appear fewer than a specified threshold (`N`) times in the training data and replaces them with the `<UNK>` token. The method returns the preprocessed corpus as a single string with tokens separated by spaces.\n"
      ],
      "metadata": {
        "id": "2uY9xJuIop3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabular = {}\n",
        "count_threshold = 2\n",
        "ngram_size = 3\n",
        "\n",
        "def prepare_data(infile, operation):\n",
        "        global vocabular # Explicitly use the global variable\n",
        "        corpus = \"\"\n",
        "        pattern = r'^(\\/)?([^\\/\\0]+(\\/)?)+\\.[^\\/\\0]+$'\n",
        "\n",
        "        # Input handling: check if infile is a file or raw text\n",
        "        if operation or re.match(pattern, infile):\n",
        "            with open(infile, 'r', encoding=\"utf8\") as file:\n",
        "                corpus = file.read()\n",
        "        else:\n",
        "            corpus = infile\n",
        "\n",
        "        newtokens = []\n",
        "        token_counts = {}\n",
        "        unk_words = []\n",
        "\n",
        "        # Preprocess the entire corpus\n",
        "        corpus = corpus.lower()\n",
        "        corpus = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', corpus, flags=re.MULTILINE)  # Remove URLs\n",
        "\n",
        "        # Tokenize the corpus into sentences\n",
        "        sentences = sent_tokenize(corpus)\n",
        "\n",
        "        # Process each sentence\n",
        "        for sentence in sentences:\n",
        "            # Add start tokens for N-gram modeling\n",
        "            newtokens.extend(['<s>'] * (ngram_size - 1))\n",
        "\n",
        "            # Tokenize sentence into words\n",
        "            words = word_tokenize(sentence)\n",
        "\n",
        "            # Handle date tokens separately\n",
        "            for token in words:\n",
        "                dates = re.findall(r'(.*)((?:0?[1-9]|[12][0-9]|3[01])/(?:0?[1-9]|1[0-2])/(?:\\d{2}|\\d{4}))(.*)', token)\n",
        "                if dates:\n",
        "                    for items in dates:\n",
        "                        for date in items:\n",
        "                            if re.match(r\"(?:0?[1-9]|[12][0-9]|3[01])/(?:0?[1-9]|1[0-2])/(?:\\d{2}|\\d{4})\", date):\n",
        "                                newtokens.append(date)\n",
        "                                token_counts[date] = token_counts.get(date, 0) + 1\n",
        "                            else:\n",
        "                                date_tokens = word_tokenize(date)\n",
        "                                newtokens.extend(date_tokens)\n",
        "                                for x in date_tokens:\n",
        "                                    token_counts[x] = token_counts.get(x, 0) + 1\n",
        "                else:\n",
        "                    newtokens.append(token)\n",
        "                    token_counts[token] = token_counts.get(token, 0) + 1\n",
        "\n",
        "            # Add end-of-sentence token\n",
        "            newtokens.append('</s>')\n",
        "\n",
        "        # Handle unknown tokens\n",
        "        if operation:\n",
        "            for i, token in enumerate(newtokens):\n",
        "                if token_counts.get(token, 0) < count_threshold and token not in ['<s>', '</s>']:\n",
        "                    unk_words.append(token)\n",
        "                    token_counts['<UNK>'] = token_counts.get('<UNK>', 0) + 1\n",
        "                    newtokens[i] = '<UNK>'\n",
        "\n",
        "            vocabular = token_counts\n",
        "            return ' '.join(newtokens)\n",
        "        else:\n",
        "            for i, token in enumerate(newtokens):\n",
        "                if token not in vocabular and token not in ['<s>', '</s>']:\n",
        "                    unk_words.append(token)\n",
        "                    newtokens[i] = '<UNK>'\n",
        "\n",
        "            return ' '.join(newtokens), unk_words"
      ],
      "metadata": {
        "id": "7ni4mIvaqXUe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data('ngramv1.train', True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "4rSNDI3zsseX",
        "outputId": "85477eb4-5fc4-4c6a-8b97-ca4d772228b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> <s> i am sam . </s> <s> <s> i am sam . </s> <s> <s> sam i am . </s> <s> <s> that sam i am ! </s> <s> <s> that sam i am ! </s> <s> <s> i do not like that sam i am ! </s> <s> <s> do would you like green eggs and ham ? </s> <s> <s> i do not like them , sam i am . </s> <s> <s> i do not like green eggs and ham . </s> <s> <s> would you like them here or there ? </s> <s> <s> i would not like them here or there . </s> <s> <s> i would not like them anywhere . </s> <s> <s> i do not like green eggs and ham . </s> <s> <s> i do not like them , sam i am . </s> <s> <s> would you like them in a house ? </s> <s> <s> would you like <UNK> with a mouse ? </s> <s> <s> i do not like them in a house . </s> <s> <s> i do not like them with a mouse . </s> <s> <s> i do not like them here or there . </s> <s> <s> i do not like them anywhere . </s> <s> <s> i do not like green eggs and ham . </s> <s> <s> i do not like them , sam i am . </s> <s> <s> would you eat them in a box ? </s> <s> <s> would you eat them with a fox ? </s> <s> <s> not in a box . </s> <s> <s> not with a fox . </s> <s> <s> not in a house . </s> <s> <s> not with a mouse . </s> <s> <s> i would not eat them here or there . </s> <s> <s> i would not eat them anywhere . </s> <s> <s> i would not eat green eggs and ham . </s> <s> <s> i do not like them , sam i am . </s> <s> <s> would you ? </s> <s> <s> could you ? </s> <s> <s> in a car ? </s> <s> <s> eat them ! </s> <s> <s> eat them ! </s> <s> <s> here <UNK> <UNK> . </s> <s> <s> i would not , could not , in a car . </s> <s> <s> you may like them . </s> <s> <s> you will see . </s> <s> <s> you may like them in a tree ! </s> <s> <s> i would not , could not in a tree . </s> <s> <s> not in a car ! </s> <s> <s> you let me be . </s> <s> <s> i do not like them in a box . </s> <s> <s> i do not like them with a fox . </s> <s> <s> i do not like them in a house . </s> <s> <s> i do not like them with a mouse . </s> <s> <s> i do not like them here or there . </s> <s> <s> i do not like them anywhere . </s> <s> <s> i do not like green eggs and ham . </s> <s> <s> i do not like them , sam i am . </s> <s> <s> a train ! </s> <s> <s> a train ! </s> <s> <s> a train ! </s> <s> <s> a train ! </s> <s> <s> could you , would you on a train ? </s> <s> <s> not on train ! </s> <s> <s> not in a tree ! </s> <s> <s> not in a car ! </s> <s> <s> sam ! </s> <s> <s> let me be ! </s> <s> <s> i would not , could not , in a box . </s> <s> <s> i would not , could not , with a fox . </s> <s> <s> i will not eat them in a house . </s> <s> <s> i will not eat them here or there . </s> <s> <s> i will not eat them anywhere . </s> <s> <s> i do not eat <UNK> eggs and ham . </s> <s> <s> i do not like them , sam i am . </s> <s> <s> say ! </s> <s> <s> in the dark ? </s> <s> <s> here in the dark ! </s> <s> <s> would you , could you , in the dark ? </s> <s> <s> i would not , could not , in the dark . </s> <s> <s> would you could you in the rain ? </s> <s> <s> i would not , could not in the rain . </s> <s> <s> not in the dark . </s> <s> <s> not on a train . </s> <s> <s> not in a car . </s> <s> <s> not in a tree . </s> <s> <s> i do not like them , sam , you see . </s> <s> <s> not in a house . </s> <s> <s> not in a box . </s> <s> <s> not with a mouse . </s> <s> <s> not with a fox . </s> <s> <s> i will not eat them here or there . </s> <s> <s> i do not like them anywhere ! </s> <s> <s> you do not like green eggs and ham ? </s> <s> <s> i do not like them , sam i am . </s> <s> <s> could you , would you , with a goat ? </s> <s> <s> i would not , could not with a goat ! </s> <s> <s> would you , could you , on a boat ? </s> <s> <s> i could not , would not , on a boat . </s> <s> <s> i will not , will not , with a goat . </s> <s> <s> i will not eat them in the rain . </s> <s> <s> not in the dark ! </s> <s> <s> not in a tree ! </s> <s> <s> not in a car ! </s> <s> <s> you let me be ! </s> <s> <s> i do not like them in a box . </s> <s> <s> i do not like them with a fox . </s> <s> <s> i will not eat them in a house . </s> <s> <s> i do not like them with a mouse . </s> <s> <s> i do not like them here or there . </s> <s> <s> i do not like them anywhere ! </s> <s> <s> i do not like green eggs and ham ! </s> <s> <s> i do not like them , sam i am . </s> <s> <s> you do not like them . </s> <s> <s> <UNK> you say . </s> <s> <s> try them ! </s> <s> <s> try them ! </s> <s> <s> and you may . </s> <s> <s> try them and you may , i say . </s> <s> <s> sam ! </s> <s> <s> <UNK> you let me be , i will try them . </s> <s> <s> you will see . </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pYWSc-wqBdKs",
        "outputId": "a130873e-b8d9-43a4-fbdd-6c2de98ca4be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'i': 68,\n",
              " 'am': 14,\n",
              " 'sam': 17,\n",
              " '.': 72,\n",
              " 'that': 3,\n",
              " '!': 29,\n",
              " 'do': 35,\n",
              " 'not': 82,\n",
              " 'like': 41,\n",
              " 'would': 25,\n",
              " 'you': 31,\n",
              " 'green': 8,\n",
              " 'eggs': 9,\n",
              " 'and': 11,\n",
              " 'ham': 9,\n",
              " '?': 16,\n",
              " 'them': 48,\n",
              " ',': 34,\n",
              " 'here': 10,\n",
              " 'or': 8,\n",
              " 'there': 8,\n",
              " 'anywhere': 7,\n",
              " 'in': 33,\n",
              " 'a': 47,\n",
              " 'house': 7,\n",
              " 'then': 1,\n",
              " 'with': 15,\n",
              " 'mouse': 6,\n",
              " 'eat': 14,\n",
              " 'box': 6,\n",
              " 'fox': 6,\n",
              " 'could': 14,\n",
              " 'car': 6,\n",
              " 'they': 1,\n",
              " 'are': 1,\n",
              " 'may': 4,\n",
              " 'will': 11,\n",
              " 'see': 3,\n",
              " 'tree': 5,\n",
              " 'let': 4,\n",
              " 'me': 4,\n",
              " 'be': 4,\n",
              " 'train': 7,\n",
              " 'on': 5,\n",
              " 'greem': 1,\n",
              " 'say': 3,\n",
              " 'the': 9,\n",
              " 'dark': 6,\n",
              " 'rain': 3,\n",
              " 'goat': 3,\n",
              " 'boat': 2,\n",
              " 'so': 1,\n",
              " 'try': 4,\n",
              " 'if': 1,\n",
              " '<UNK>': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method: `train(infile)`\n",
        "\n",
        "This method calculates the probabilities of n-grams. It uses the `prepare_data` method for preprocessing the input.\n",
        "\n",
        "Our model takes into account the following three points:\n",
        "- Handles out-of-vocabulary words.\n",
        "- Applies smoothing (uses add-k smoothing in this calculation).\n",
        "- Converts probabilities to logarithms, to avoid floating-point overflow issues."
      ],
      "metadata": {
        "id": "-GO_Jdmpo465"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts = {}\n",
        "nminus1_gram_counts = {}\n",
        "k = 0.01\n",
        "ngram_probabilities = {}\n",
        "def train(infile=None):\n",
        "    global n_gram_counts, nminus1_gram_counts, ngram_probabilities\n",
        "    # Preprocess the input corpus\n",
        "    corpus = prepare_data(infile, True)\n",
        "\n",
        "    # Extract sentences from the preprocessed corpus\n",
        "    sentences = re.findall(r\"<s>.+?</s>\", corpus)\n",
        "\n",
        "    # Step 1: Count n-grams and (n-1)-grams\n",
        "    for captured_sentence in sentences:\n",
        "        tokens = captured_sentence.split(\" \")\n",
        "\n",
        "        for i in range(len(tokens) - (ngram_size - 1)):\n",
        "            # Extract n-gram and n-1-gram\n",
        "            n_gram = tokens[i:i + ngram_size]\n",
        "            n_gram_tuple = tuple(n_gram)\n",
        "            nminus1_gram_tuple = tuple(n_gram[:-1])\n",
        "\n",
        "            # Update counts\n",
        "            n_gram_counts[n_gram_tuple] = n_gram_counts.get(n_gram_tuple, 0) + 1\n",
        "            nminus1_gram_counts[nminus1_gram_tuple] = nminus1_gram_counts.get(nminus1_gram_tuple, 0) + 1\n",
        "\n",
        "    # Step 2: Add-k smoothing\n",
        "    vocab_size = len(vocabular)\n",
        "    ngram_probabilities = {}\n",
        "\n",
        "    for ngram, count in n_gram_counts.items():\n",
        "        n_minus1_gram = ngram[:-1]\n",
        "        count_nminus1 = nminus1_gram_counts.get(n_minus1_gram, 0)\n",
        "\n",
        "        # Add-k smoothing: P(wi | wi-1, ..., wi-(n-1)) = (count(wi-1,...,wi) + k) / (count(wi-1,...,wi-(n-1)) + k * V)\n",
        "        smoothed_prob = (count + k) / (count_nminus1 + k * vocab_size)\n",
        "\n",
        "        # Logarithmic probabilities\n",
        "        ngram_probabilities[ngram] = math.log(smoothed_prob)\n",
        "\n",
        "    # Save the probabilities for the model\n",
        "    ngram_probabilities = ngram_probabilities"
      ],
      "metadata": {
        "id": "2VUTUstEtb4Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('ngramv1.train')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cj3Moc7OujYY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-EsnhZMu6fG",
        "outputId": "834500db-3574-4571-d909-1fc3c1232527",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('<s>', '<s>', 'i'): 54,\n",
              " ('<s>', 'i', 'am'): 2,\n",
              " ('i', 'am', 'sam'): 2,\n",
              " ('am', 'sam', '.'): 2,\n",
              " ('sam', '.', '</s>'): 2,\n",
              " ('<s>', '<s>', 'sam'): 3,\n",
              " ('<s>', 'sam', 'i'): 1,\n",
              " ('sam', 'i', 'am'): 12,\n",
              " ('i', 'am', '.'): 9,\n",
              " ('am', '.', '</s>'): 9,\n",
              " ('<s>', '<s>', 'that'): 2,\n",
              " ('<s>', 'that', 'sam'): 2,\n",
              " ('that', 'sam', 'i'): 3,\n",
              " ('i', 'am', '!'): 3,\n",
              " ('am', '!', '</s>'): 3,\n",
              " ('<s>', 'i', 'do'): 32,\n",
              " ('i', 'do', 'not'): 32,\n",
              " ('do', 'not', 'like'): 33,\n",
              " ('not', 'like', 'that'): 1,\n",
              " ('like', 'that', 'sam'): 1,\n",
              " ('<s>', '<s>', 'do'): 1,\n",
              " ('<s>', 'do', 'would'): 1,\n",
              " ('do', 'would', 'you'): 1,\n",
              " ('would', 'you', 'like'): 4,\n",
              " ('you', 'like', 'green'): 1,\n",
              " ('like', 'green', 'eggs'): 7,\n",
              " ('green', 'eggs', 'and'): 8,\n",
              " ('eggs', 'and', 'ham'): 9,\n",
              " ('and', 'ham', '?'): 2,\n",
              " ('ham', '?', '</s>'): 2,\n",
              " ('not', 'like', 'them'): 28,\n",
              " ('like', 'them', ','): 9,\n",
              " ('them', ',', 'sam'): 9,\n",
              " (',', 'sam', 'i'): 8,\n",
              " ('not', 'like', 'green'): 6,\n",
              " ('and', 'ham', '.'): 6,\n",
              " ('ham', '.', '</s>'): 6,\n",
              " ('<s>', '<s>', 'would'): 9,\n",
              " ('<s>', 'would', 'you'): 9,\n",
              " ('you', 'like', 'them'): 2,\n",
              " ('like', 'them', 'here'): 5,\n",
              " ('them', 'here', 'or'): 8,\n",
              " ('here', 'or', 'there'): 8,\n",
              " ('or', 'there', '?'): 1,\n",
              " ('there', '?', '</s>'): 1,\n",
              " ('<s>', 'i', 'would'): 12,\n",
              " ('i', 'would', 'not'): 12,\n",
              " ('would', 'not', 'like'): 2,\n",
              " ('or', 'there', '.'): 7,\n",
              " ('there', '.', '</s>'): 7,\n",
              " ('like', 'them', 'anywhere'): 5,\n",
              " ('them', 'anywhere', '.'): 5,\n",
              " ('anywhere', '.', '</s>'): 5,\n",
              " ('like', 'them', 'in'): 6,\n",
              " ('them', 'in', 'a'): 9,\n",
              " ('in', 'a', 'house'): 7,\n",
              " ('a', 'house', '?'): 1,\n",
              " ('house', '?', '</s>'): 1,\n",
              " ('you', 'like', '<UNK>'): 1,\n",
              " ('like', '<UNK>', 'with'): 1,\n",
              " ('<UNK>', 'with', 'a'): 1,\n",
              " ('with', 'a', 'mouse'): 6,\n",
              " ('a', 'mouse', '?'): 1,\n",
              " ('mouse', '?', '</s>'): 1,\n",
              " ('a', 'house', '.'): 6,\n",
              " ('house', '.', '</s>'): 6,\n",
              " ('like', 'them', 'with'): 5,\n",
              " ('them', 'with', 'a'): 6,\n",
              " ('a', 'mouse', '.'): 5,\n",
              " ('mouse', '.', '</s>'): 5,\n",
              " ('would', 'you', 'eat'): 2,\n",
              " ('you', 'eat', 'them'): 2,\n",
              " ('eat', 'them', 'in'): 4,\n",
              " ('in', 'a', 'box'): 6,\n",
              " ('a', 'box', '?'): 1,\n",
              " ('box', '?', '</s>'): 1,\n",
              " ('eat', 'them', 'with'): 1,\n",
              " ('with', 'a', 'fox'): 6,\n",
              " ('a', 'fox', '?'): 1,\n",
              " ('fox', '?', '</s>'): 1,\n",
              " ('<s>', '<s>', 'not'): 19,\n",
              " ('<s>', 'not', 'in'): 13,\n",
              " ('not', 'in', 'a'): 12,\n",
              " ('a', 'box', '.'): 5,\n",
              " ('box', '.', '</s>'): 5,\n",
              " ('<s>', 'not', 'with'): 4,\n",
              " ('not', 'with', 'a'): 5,\n",
              " ('a', 'fox', '.'): 5,\n",
              " ('fox', '.', '</s>'): 5,\n",
              " ('would', 'not', 'eat'): 3,\n",
              " ('not', 'eat', 'them'): 8,\n",
              " ('eat', 'them', 'here'): 3,\n",
              " ('eat', 'them', 'anywhere'): 2,\n",
              " ('not', 'eat', 'green'): 1,\n",
              " ('eat', 'green', 'eggs'): 1,\n",
              " ('would', 'you', '?'): 1,\n",
              " ('you', '?', '</s>'): 2,\n",
              " ('<s>', '<s>', 'could'): 3,\n",
              " ('<s>', 'could', 'you'): 3,\n",
              " ('could', 'you', '?'): 1,\n",
              " ('<s>', '<s>', 'in'): 2,\n",
              " ('<s>', 'in', 'a'): 1,\n",
              " ('in', 'a', 'car'): 6,\n",
              " ('a', 'car', '?'): 1,\n",
              " ('car', '?', '</s>'): 1,\n",
              " ('<s>', '<s>', 'eat'): 2,\n",
              " ('<s>', 'eat', 'them'): 2,\n",
              " ('eat', 'them', '!'): 2,\n",
              " ('them', '!', '</s>'): 4,\n",
              " ('<s>', '<s>', 'here'): 2,\n",
              " ('<s>', 'here', '<UNK>'): 1,\n",
              " ('here', '<UNK>', '<UNK>'): 1,\n",
              " ('<UNK>', '<UNK>', '.'): 1,\n",
              " ('<UNK>', '.', '</s>'): 1,\n",
              " ('would', 'not', ','): 8,\n",
              " ('not', ',', 'could'): 7,\n",
              " (',', 'could', 'not'): 7,\n",
              " ('could', 'not', ','): 5,\n",
              " ('not', ',', 'in'): 3,\n",
              " (',', 'in', 'a'): 2,\n",
              " ('a', 'car', '.'): 2,\n",
              " ('car', '.', '</s>'): 2,\n",
              " ('<s>', '<s>', 'you'): 8,\n",
              " ('<s>', 'you', 'may'): 2,\n",
              " ('you', 'may', 'like'): 2,\n",
              " ('may', 'like', 'them'): 2,\n",
              " ('like', 'them', '.'): 2,\n",
              " ('them', '.', '</s>'): 3,\n",
              " ('<s>', 'you', 'will'): 2,\n",
              " ('you', 'will', 'see'): 2,\n",
              " ('will', 'see', '.'): 2,\n",
              " ('see', '.', '</s>'): 3,\n",
              " ('in', 'a', 'tree'): 5,\n",
              " ('a', 'tree', '!'): 3,\n",
              " ('tree', '!', '</s>'): 3,\n",
              " ('could', 'not', 'in'): 2,\n",
              " ('a', 'tree', '.'): 2,\n",
              " ('tree', '.', '</s>'): 2,\n",
              " ('a', 'car', '!'): 3,\n",
              " ('car', '!', '</s>'): 3,\n",
              " ('<s>', 'you', 'let'): 2,\n",
              " ('you', 'let', 'me'): 3,\n",
              " ('let', 'me', 'be'): 4,\n",
              " ('me', 'be', '.'): 1,\n",
              " ('be', '.', '</s>'): 1,\n",
              " ('<s>', '<s>', 'a'): 4,\n",
              " ('<s>', 'a', 'train'): 4,\n",
              " ('a', 'train', '!'): 4,\n",
              " ('train', '!', '</s>'): 5,\n",
              " ('could', 'you', ','): 4,\n",
              " ('you', ',', 'would'): 2,\n",
              " (',', 'would', 'you'): 2,\n",
              " ('would', 'you', 'on'): 1,\n",
              " ('you', 'on', 'a'): 1,\n",
              " ('on', 'a', 'train'): 2,\n",
              " ('a', 'train', '?'): 1,\n",
              " ('train', '?', '</s>'): 1,\n",
              " ('<s>', 'not', 'on'): 2,\n",
              " ('not', 'on', 'train'): 1,\n",
              " ('on', 'train', '!'): 1,\n",
              " ('<s>', 'sam', '!'): 2,\n",
              " ('sam', '!', '</s>'): 2,\n",
              " ('<s>', '<s>', 'let'): 1,\n",
              " ('<s>', 'let', 'me'): 1,\n",
              " ('me', 'be', '!'): 2,\n",
              " ('be', '!', '</s>'): 2,\n",
              " ('not', ',', 'with'): 2,\n",
              " (',', 'with', 'a'): 3,\n",
              " ('<s>', 'i', 'will'): 7,\n",
              " ('i', 'will', 'not'): 7,\n",
              " ('will', 'not', 'eat'): 6,\n",
              " ('do', 'not', 'eat'): 1,\n",
              " ('not', 'eat', '<UNK>'): 1,\n",
              " ('eat', '<UNK>', 'eggs'): 1,\n",
              " ('<UNK>', 'eggs', 'and'): 1,\n",
              " ('<s>', '<s>', 'say'): 1,\n",
              " ('<s>', 'say', '!'): 1,\n",
              " ('say', '!', '</s>'): 1,\n",
              " ('<s>', 'in', 'the'): 1,\n",
              " ('in', 'the', 'dark'): 6,\n",
              " ('the', 'dark', '?'): 2,\n",
              " ('dark', '?', '</s>'): 2,\n",
              " ('<s>', 'here', 'in'): 1,\n",
              " ('here', 'in', 'the'): 1,\n",
              " ('the', 'dark', '!'): 2,\n",
              " ('dark', '!', '</s>'): 2,\n",
              " ('would', 'you', ','): 3,\n",
              " ('you', ',', 'could'): 2,\n",
              " (',', 'could', 'you'): 2,\n",
              " ('you', ',', 'in'): 1,\n",
              " (',', 'in', 'the'): 2,\n",
              " ('the', 'dark', '.'): 2,\n",
              " ('dark', '.', '</s>'): 2,\n",
              " ('would', 'you', 'could'): 1,\n",
              " ('you', 'could', 'you'): 1,\n",
              " ('could', 'you', 'in'): 1,\n",
              " ('you', 'in', 'the'): 1,\n",
              " ('in', 'the', 'rain'): 3,\n",
              " ('the', 'rain', '?'): 1,\n",
              " ('rain', '?', '</s>'): 1,\n",
              " ('not', 'in', 'the'): 3,\n",
              " ('the', 'rain', '.'): 2,\n",
              " ('rain', '.', '</s>'): 2,\n",
              " ('not', 'on', 'a'): 1,\n",
              " ('a', 'train', '.'): 1,\n",
              " ('train', '.', '</s>'): 1,\n",
              " (',', 'sam', ','): 1,\n",
              " ('sam', ',', 'you'): 1,\n",
              " (',', 'you', 'see'): 1,\n",
              " ('you', 'see', '.'): 1,\n",
              " ('them', 'anywhere', '!'): 2,\n",
              " ('anywhere', '!', '</s>'): 2,\n",
              " ('<s>', 'you', 'do'): 2,\n",
              " ('you', 'do', 'not'): 2,\n",
              " ('you', ',', 'with'): 1,\n",
              " ('with', 'a', 'goat'): 3,\n",
              " ('a', 'goat', '?'): 1,\n",
              " ('goat', '?', '</s>'): 1,\n",
              " ('could', 'not', 'with'): 1,\n",
              " ('a', 'goat', '!'): 1,\n",
              " ('goat', '!', '</s>'): 1,\n",
              " ('you', ',', 'on'): 1,\n",
              " (',', 'on', 'a'): 2,\n",
              " ('on', 'a', 'boat'): 2,\n",
              " ('a', 'boat', '?'): 1,\n",
              " ('boat', '?', '</s>'): 1,\n",
              " ('<s>', 'i', 'could'): 1,\n",
              " ('i', 'could', 'not'): 1,\n",
              " ('not', ',', 'would'): 1,\n",
              " (',', 'would', 'not'): 1,\n",
              " ('not', ',', 'on'): 1,\n",
              " ('a', 'boat', '.'): 1,\n",
              " ('boat', '.', '</s>'): 1,\n",
              " ('will', 'not', ','): 2,\n",
              " ('not', ',', 'will'): 1,\n",
              " (',', 'will', 'not'): 1,\n",
              " ('a', 'goat', '.'): 1,\n",
              " ('goat', '.', '</s>'): 1,\n",
              " ('them', 'in', 'the'): 1,\n",
              " ('and', 'ham', '!'): 1,\n",
              " ('ham', '!', '</s>'): 1,\n",
              " ('<s>', '<s>', '<UNK>'): 2,\n",
              " ('<s>', '<UNK>', 'you'): 2,\n",
              " ('<UNK>', 'you', 'say'): 1,\n",
              " ('you', 'say', '.'): 1,\n",
              " ('say', '.', '</s>'): 2,\n",
              " ('<s>', '<s>', 'try'): 3,\n",
              " ('<s>', 'try', 'them'): 3,\n",
              " ('try', 'them', '!'): 2,\n",
              " ('<s>', '<s>', 'and'): 1,\n",
              " ('<s>', 'and', 'you'): 1,\n",
              " ('and', 'you', 'may'): 2,\n",
              " ('you', 'may', '.'): 1,\n",
              " ('may', '.', '</s>'): 1,\n",
              " ('try', 'them', 'and'): 1,\n",
              " ('them', 'and', 'you'): 1,\n",
              " ('you', 'may', ','): 1,\n",
              " ('may', ',', 'i'): 1,\n",
              " (',', 'i', 'say'): 1,\n",
              " ('i', 'say', '.'): 1,\n",
              " ('<UNK>', 'you', 'let'): 1,\n",
              " ('me', 'be', ','): 1,\n",
              " ('be', ',', 'i'): 1,\n",
              " (',', 'i', 'will'): 1,\n",
              " ('i', 'will', 'try'): 1,\n",
              " ('will', 'try', 'them'): 1,\n",
              " ('try', 'them', '.'): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bfe9Oc2G9Dzu",
        "outputId": "f0e411fe-74ea-4456-9bf8-7fbeb5aa3aae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('<s>', '<s>', 'i'): -0.7776945603812787,\n",
              " ('<s>', 'i', 'am'): -3.3009829902080954,\n",
              " ('i', 'am', 'sam'): -1.9794562715465174,\n",
              " ('am', 'sam', '.'): -0.23795863709935042,\n",
              " ('sam', '.', '</s>'): -0.23795863709935042,\n",
              " ('<s>', '<s>', 'sam'): -3.6649236962252942,\n",
              " ('<s>', 'sam', 'i'): -1.2569972726341563,\n",
              " ('sam', 'i', 'am'): -0.043981029485900776,\n",
              " ('i', 'am', '.'): -0.47925592199725514,\n",
              " ('am', '.', '</s>'): -0.05820608287239237,\n",
              " ('<s>', '<s>', 'that'): -4.068729052915094,\n",
              " ('<s>', 'that', 'sam'): -0.23795863709935042,\n",
              " ('that', 'sam', 'i'): -0.1650075247265401,\n",
              " ('i', 'am', '!'): -1.5756509148567173,\n",
              " ('am', '!', '</s>'): -0.1650075247265401,\n",
              " ('<s>', 'i', 'do'): -0.5330693582973082,\n",
              " ('i', 'do', 'not'): -0.016729014672806554,\n",
              " ('do', 'not', 'like'): -0.04559700434857725,\n",
              " ('not', 'like', 'that'): -3.560989825396082,\n",
              " ('like', 'that', 'sam'): -0.42830460007798715,\n",
              " ('<s>', '<s>', 'do'): -4.75691344413291,\n",
              " ('<s>', 'do', 'would'): -0.42830460007798715,\n",
              " ('do', 'would', 'you'): -0.42830460007798715,\n",
              " ('would', 'you', 'like'): -1.140929424259315,\n",
              " ('you', 'like', 'green'): -1.5051769021096908,\n",
              " ('like', 'green', 'eggs'): -0.0742098622144346,\n",
              " ('green', 'eggs', 'and'): -0.06524052186840108,\n",
              " ('eggs', 'and', 'ham'): -0.05820608287239237,\n",
              " ('and', 'ham', '?'): -1.5584064324216547,\n",
              " ('ham', '?', '</s>'): -0.23795863709935042,\n",
              " ('not', 'like', 'them'): -0.23837856697723264,\n",
              " ('like', 'them', ','): -1.2844422970343317,\n",
              " ('them', ',', 'sam'): -0.05820608287239237,\n",
              " (',', 'sam', 'i'): -0.17585039341237108,\n",
              " ('not', 'like', 'green'): -1.7775154077021336,\n",
              " ('and', 'ham', '.'): -0.4631164059455227,\n",
              " ('ham', '.', '</s>'): -0.08604030110004429,\n",
              " ('<s>', '<s>', 'would'): -2.568528703365832,\n",
              " ('<s>', 'would', 'you'): -0.05820608287239237,\n",
              " ('you', 'like', 'them'): -0.8169925108918747,\n",
              " ('like', 'them', 'here'): -1.8713414535578048,\n",
              " ('them', 'here', 'or'): -0.06524052186840108,\n",
              " ('here', 'or', 'there'): -0.06524052186840108,\n",
              " ('or', 'there', '?'): -2.1359809520955007,\n",
              " ('there', '?', '</s>'): -0.42830460007798715,\n",
              " ('<s>', 'i', 'would'): -1.5133780761871876,\n",
              " ('i', 'would', 'not'): -0.043981029485900776,\n",
              " ('would', 'not', 'like'): -1.9082518252547256,\n",
              " ('or', 'there', '.'): -0.19859358190217022,\n",
              " ('there', '.', '</s>'): -0.0742098622144346,\n",
              " ('like', 'them', 'anywhere'): -1.8713414535578048,\n",
              " ('them', 'anywhere', '.'): -0.4101116481641599,\n",
              " ('anywhere', '.', '</s>'): -0.10236201266156972,\n",
              " ('like', 'them', 'in'): -1.689352620107462,\n",
              " ('them', 'in', 'a'): -0.15779078830182902,\n",
              " ('in', 'a', 'house'): -1.253374153194031,\n",
              " ('a', 'house', '?'): -2.011597232407765,\n",
              " ('house', '?', '</s>'): -0.42830460007798715,\n",
              " ('you', 'like', '<UNK>'): -1.5051769021096908,\n",
              " ('like', '<UNK>', 'with'): -0.42830460007798715,\n",
              " ('<UNK>', 'with', 'a'): -0.42830460007798715,\n",
              " ('with', 'a', 'mouse'): -0.950635890078127,\n",
              " ('a', 'mouse', '?'): -1.8695147187939924,\n",
              " ('mouse', '?', '</s>'): -0.42830460007798715,\n",
              " ('a', 'house', '.'): -0.22812281471381712,\n",
              " ('house', '.', '</s>'): -0.08604030110004429,\n",
              " ('like', 'them', 'with'): -1.8713414535578048,\n",
              " ('them', 'with', 'a'): -0.08604030110004429,\n",
              " ('a', 'mouse', '.'): -0.2680291345503871,\n",
              " ('mouse', '.', '</s>'): -0.10236201266156972,\n",
              " ('would', 'you', 'eat'): -1.8315859435068087,\n",
              " ('you', 'eat', 'them'): -0.23795863709935042,\n",
              " ('eat', 'them', 'in'): -1.140929424259315,\n",
              " ('in', 'a', 'box'): -1.4072871056934135,\n",
              " ('a', 'box', '?'): -1.8695147187939924,\n",
              " ('box', '?', '</s>'): -0.42830460007798715,\n",
              " ('eat', 'them', 'with'): -2.5197703347246247,\n",
              " ('with', 'a', 'fox'): -0.950635890078127,\n",
              " ('a', 'fox', '?'): -1.8695147187939924,\n",
              " ('fox', '?', '</s>'): -0.42830460007798715,\n",
              " ('<s>', '<s>', 'not'): -1.8218986184857406,\n",
              " ('<s>', 'not', 'in'): -0.4072569939069609,\n",
              " ('not', 'in', 'a'): -0.2583210025333511,\n",
              " ('a', 'box', '.'): -0.2680291345503871,\n",
              " ('box', '.', '</s>'): -0.10236201266156972,\n",
              " ('<s>', 'not', 'with'): -1.584184045112897,\n",
              " ('not', 'with', 'a'): -0.10236201266156972,\n",
              " ('a', 'fox', '.'): -0.2680291345503871,\n",
              " ('fox', '.', '</s>'): -0.10236201266156972,\n",
              " ('would', 'not', 'eat'): -1.5044464685649257,\n",
              " ('not', 'eat', 'them'): -0.2754350988418077,\n",
              " ('eat', 'them', 'here'): -1.4277805868170086,\n",
              " ('eat', 'them', 'anywhere'): -1.8315859435068087,\n",
              " ('not', 'eat', 'green'): -2.3461755290689075,\n",
              " ('eat', 'green', 'eggs'): -0.42830460007798715,\n",
              " ('would', 'you', '?'): -2.5197703347246247,\n",
              " ('you', '?', '</s>'): -0.23795863709935042,\n",
              " ('<s>', '<s>', 'could'): -3.6649236962252942,\n",
              " ('<s>', 'could', 'you'): -0.1650075247265401,\n",
              " ('could', 'you', '?'): -1.8695147187939924,\n",
              " ('<s>', '<s>', 'in'): -4.068729052915094,\n",
              " ('<s>', 'in', 'a'): -0.9261430283171666,\n",
              " ('in', 'a', 'car'): -1.4072871056934135,\n",
              " ('a', 'car', '?'): -1.8695147187939924,\n",
              " ('car', '?', '</s>'): -0.42830460007798715,\n",
              " ('<s>', '<s>', 'eat'): -4.068729052915094,\n",
              " ('<s>', 'eat', 'them'): -0.23795863709935042,\n",
              " ('eat', 'them', '!'): -1.8315859435068087,\n",
              " ('them', '!', '</s>'): -0.12633599164438125,\n",
              " ('<s>', '<s>', 'here'): -4.068729052915094,\n",
              " ('<s>', 'here', '<UNK>'): -0.9261430283171666,\n",
              " ('here', '<UNK>', '<UNK>'): -0.42830460007798715,\n",
              " ('<UNK>', '<UNK>', '.'): -0.42830460007798715,\n",
              " ('<UNK>', '.', '</s>'): -0.42830460007798715,\n",
              " ('would', 'not', ','): -0.5256957862454421,\n",
              " ('not', ',', 'could'): -0.7967229375787446,\n",
              " (',', 'could', 'not'): -0.3092034534461402,\n",
              " ('could', 'not', ','): -0.5344953678518956,\n",
              " ('not', ',', 'in'): -1.642120559864459,\n",
              " (',', 'in', 'a'): -0.8169925108918747,\n",
              " ('a', 'car', '.'): -1.1813303275761762,\n",
              " ('car', '.', '</s>'): -0.23795863709935042,\n",
              " ('<s>', '<s>', 'you'): -2.6861730139058104,\n",
              " ('<s>', 'you', 'may'): -1.4477965608776846,\n",
              " ('you', 'may', 'like'): -0.8169925108918747,\n",
              " ('may', 'like', 'them'): -0.23795863709935042,\n",
              " ('like', 'them', '.'): -2.784642646583594,\n",
              " ('them', '.', '</s>'): -0.1650075247265401,\n",
              " ('<s>', 'you', 'will'): -1.4477965608776846,\n",
              " ('you', 'will', 'see'): -0.23795863709935042,\n",
              " ('will', 'see', '.'): -0.23795863709935042,\n",
              " ('see', '.', '</s>'): -0.1650075247265401,\n",
              " ('in', 'a', 'tree'): -1.589275939143756,\n",
              " ('a', 'tree', '!'): -0.6118578489975588,\n",
              " ('tree', '!', '</s>'): -0.1650075247265401,\n",
              " ('could', 'not', 'in'): -1.4477965608776846,\n",
              " ('a', 'tree', '.'): -1.015663205687359,\n",
              " ('tree', '.', '</s>'): -0.23795863709935042,\n",
              " ('a', 'car', '!'): -0.7775249708863762,\n",
              " ('car', '!', '</s>'): -0.1650075247265401,\n",
              " ('<s>', 'you', 'let'): -1.4477965608776846,\n",
              " ('you', 'let', 'me'): -0.1650075247265401,\n",
              " ('let', 'me', 'be'): -0.12633599164438125,\n",
              " ('me', 'be', '.'): -1.5051769021096908,\n",
              " ('be', '.', '</s>'): -0.42830460007798715,\n",
              " ('<s>', '<s>', 'a'): -3.3780725336676007,\n",
              " ('<s>', 'a', 'train'): -0.12633599164438125,\n",
              " ('a', 'train', '!'): -0.4906738083286828,\n",
              " ('train', '!', '</s>'): -0.10236201266156972,\n",
              " ('could', 'you', ','): -0.4906738083286828,\n",
              " ('you', ',', 'would'): -1.323412841189949,\n",
              " (',', 'would', 'you'): -0.56881288141634,\n",
              " ('would', 'you', 'on'): -2.5197703347246247,\n",
              " ('you', 'on', 'a'): -0.42830460007798715,\n",
              " ('on', 'a', 'train'): -0.8169925108918747,\n",
              " ('a', 'train', '?'): -1.8695147187939924,\n",
              " ('train', '?', '</s>'): -0.42830460007798715,\n",
              " ('<s>', 'not', 'on'): -2.2748405643603906,\n",
              " ('not', 'on', 'train'): -0.9261430283171666,\n",
              " ('on', 'train', '!'): -0.42830460007798715,\n",
              " ('<s>', 'sam', '!'): -0.56881288141634,\n",
              " ('sam', '!', '</s>'): -0.23795863709935042,\n",
              " ('<s>', '<s>', 'let'): -4.75691344413291,\n",
              " ('<s>', 'let', 'me'): -0.42830460007798715,\n",
              " ('me', 'be', '!'): -0.8169925108918747,\n",
              " ('be', '!', '</s>'): -0.23795863709935042,\n",
              " ('not', ',', 'with'): -2.045925916554259,\n",
              " (',', 'with', 'a'): -0.1650075247265401,\n",
              " ('<s>', 'i', 'will'): -2.051780011232581,\n",
              " ('i', 'will', 'not'): -0.19859358190217022,\n",
              " ('will', 'not', 'eat'): -0.3525065344015527,\n",
              " ('do', 'not', 'eat'): -3.5324572193605106,\n",
              " ('not', 'eat', '<UNK>'): -2.3461755290689075,\n",
              " ('eat', '<UNK>', 'eggs'): -0.42830460007798715,\n",
              " ('<UNK>', 'eggs', 'and'): -0.42830460007798715,\n",
              " ('<s>', '<s>', 'say'): -4.75691344413291,\n",
              " ('<s>', 'say', '!'): -0.42830460007798715,\n",
              " ('say', '!', '</s>'): -0.42830460007798715,\n",
              " ('<s>', 'in', 'the'): -0.9261430283171666,\n",
              " ('in', 'the', 'dark'): -0.4631164059455227,\n",
              " ('the', 'dark', '?'): -1.1813303275761762,\n",
              " ('dark', '?', '</s>'): -0.23795863709935042,\n",
              " ('<s>', 'here', 'in'): -0.9261430283171666,\n",
              " ('here', 'in', 'the'): -0.42830460007798715,\n",
              " ('the', 'dark', '!'): -1.1813303275761762,\n",
              " ('dark', '!', '</s>'): -0.23795863709935042,\n",
              " ('would', 'you', ','): -1.4277805868170086,\n",
              " ('you', ',', 'could'): -1.323412841189949,\n",
              " (',', 'could', 'you'): -1.5584064324216547,\n",
              " ('you', ',', 'in'): -2.011597232407765,\n",
              " (',', 'in', 'the'): -0.8169925108918747,\n",
              " ('the', 'dark', '.'): -1.1813303275761762,\n",
              " ('dark', '.', '</s>'): -0.23795863709935042,\n",
              " ('would', 'you', 'could'): -2.5197703347246247,\n",
              " ('you', 'could', 'you'): -0.42830460007798715,\n",
              " ('could', 'you', 'in'): -1.8695147187939924,\n",
              " ('you', 'in', 'the'): -0.42830460007798715,\n",
              " ('in', 'the', 'rain'): -1.1546010757318546,\n",
              " ('the', 'rain', '?'): -1.2569972726341563,\n",
              " ('rain', '?', '</s>'): -0.42830460007798715,\n",
              " ('not', 'in', 'the'): -1.642120559864459,\n",
              " ('the', 'rain', '.'): -0.56881288141634,\n",
              " ('rain', '.', '</s>'): -0.23795863709935042,\n",
              " ('not', 'on', 'a'): -0.9261430283171666,\n",
              " ('a', 'train', '.'): -1.8695147187939924,\n",
              " ('train', '.', '</s>'): -0.42830460007798715,\n",
              " (',', 'sam', ','): -2.2465908236394707,\n",
              " ('sam', ',', 'you'): -0.42830460007798715,\n",
              " (',', 'you', 'see'): -0.42830460007798715,\n",
              " ('you', 'see', '.'): -0.42830460007798715,\n",
              " ('them', 'anywhere', '!'): -1.323412841189949,\n",
              " ('anywhere', '!', '</s>'): -0.23795863709935042,\n",
              " ('<s>', 'you', 'do'): -1.4477965608776846,\n",
              " ('you', 'do', 'not'): -0.23795863709935042,\n",
              " ('you', ',', 'with'): -2.011597232407765,\n",
              " ('with', 'a', 'goat'): -1.642120559864459,\n",
              " ('a', 'goat', '?'): -1.2569972726341563,\n",
              " ('goat', '?', '</s>'): -0.42830460007798715,\n",
              " ('could', 'not', 'with'): -2.1359809520955007,\n",
              " ('a', 'goat', '!'): -1.2569972726341563,\n",
              " ('goat', '!', '</s>'): -0.42830460007798715,\n",
              " ('you', ',', 'on'): -2.011597232407765,\n",
              " (',', 'on', 'a'): -0.23795863709935042,\n",
              " ('on', 'a', 'boat'): -0.8169925108918747,\n",
              " ('a', 'boat', '?'): -0.9261430283171666,\n",
              " ('boat', '?', '</s>'): -0.42830460007798715,\n",
              " ('<s>', 'i', 'could'): -3.989167381425912,\n",
              " ('i', 'could', 'not'): -0.42830460007798715,\n",
              " ('not', ',', 'would'): -2.734110307772075,\n",
              " (',', 'would', 'not'): -1.2569972726341563,\n",
              " ('not', ',', 'on'): -2.734110307772075,\n",
              " ('a', 'boat', '.'): -0.9261430283171666,\n",
              " ('boat', '.', '</s>'): -0.42830460007798715,\n",
              " ('will', 'not', ','): -1.4477965608776846,\n",
              " ('not', ',', 'will'): -2.734110307772075,\n",
              " (',', 'will', 'not'): -0.42830460007798715,\n",
              " ('a', 'goat', '.'): -1.2569972726341563,\n",
              " ('goat', '.', '</s>'): -0.42830460007798715,\n",
              " ('them', 'in', 'the'): -2.3461755290689075,\n",
              " ('and', 'ham', '!'): -2.2465908236394707,\n",
              " ('ham', '!', '</s>'): -0.42830460007798715,\n",
              " ('<s>', '<s>', '<UNK>'): -4.068729052915094,\n",
              " ('<s>', '<UNK>', 'you'): -0.23795863709935042,\n",
              " ('<UNK>', 'you', 'say'): -0.9261430283171666,\n",
              " ('you', 'say', '.'): -0.42830460007798715,\n",
              " ('say', '.', '</s>'): -0.23795863709935042,\n",
              " ('<s>', '<s>', 'try'): -3.6649236962252942,\n",
              " ('<s>', 'try', 'them'): -0.1650075247265401,\n",
              " ('try', 'them', '!'): -0.8169925108918747,\n",
              " ('<s>', '<s>', 'and'): -4.75691344413291,\n",
              " ('<s>', 'and', 'you'): -0.42830460007798715,\n",
              " ('and', 'you', 'may'): -0.23795863709935042,\n",
              " ('you', 'may', '.'): -1.5051769021096908,\n",
              " ('may', '.', '</s>'): -0.42830460007798715,\n",
              " ('try', 'them', 'and'): -1.5051769021096908,\n",
              " ('them', 'and', 'you'): -0.42830460007798715,\n",
              " ('you', 'may', ','): -1.5051769021096908,\n",
              " ('may', ',', 'i'): -0.42830460007798715,\n",
              " (',', 'i', 'say'): -0.9261430283171666,\n",
              " ('i', 'say', '.'): -0.42830460007798715,\n",
              " ('<UNK>', 'you', 'let'): -0.9261430283171666,\n",
              " ('me', 'be', ','): -1.5051769021096908,\n",
              " ('be', ',', 'i'): -0.42830460007798715,\n",
              " (',', 'i', 'will'): -0.9261430283171666,\n",
              " ('i', 'will', 'try'): -2.1359809520955007,\n",
              " ('will', 'try', 'them'): -0.42830460007798715,\n",
              " ('try', 'them', '.'): -1.5051769021096908}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method: `predict_ngram(sentence)`\n",
        "\n",
        "This method takes a sentence (as a string) as input, uses the `prepare_data` method for preprocessing, and then calculates the probability of the sentence using the n-gram language model, depending on the value of the `ngram_size` parameter."
      ],
      "metadata": {
        "id": "SCZk3n3vpJKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ngram(sentence):\n",
        "      log_proba = 0\n",
        "      preprocessed_corpus, _ = prepare_data(sentence, False)\n",
        "      sentences = re.findall(r\"<s>.+?</s>\", preprocessed_corpus)\n",
        "\n",
        "      for captured_sentence in sentences:\n",
        "          tokens = captured_sentence.split(\" \")\n",
        "\n",
        "          # Calculate the log probability for each n-gram in the sentence\n",
        "          for i in range(len(tokens) - (ngram_size - 1)):\n",
        "              n_gram = tokens[i:i + ngram_size]\n",
        "              n_gram_tuple = tuple(n_gram)\n",
        "              nminus1_gram_tuple = tuple(n_gram[:-1])\n",
        "\n",
        "              # Use add-k smoothing to calculate log probability\n",
        "              count_n_gram = n_gram_counts.get(n_gram_tuple, 0)\n",
        "              count_nminus1_gram = nminus1_gram_counts.get(nminus1_gram_tuple, 0)\n",
        "\n",
        "              smoothed_prob = (count_n_gram + k) / (count_nminus1_gram + k * len(vocabular))\n",
        "              log_proba += math.log(smoothed_prob)\n",
        "\n",
        "      return log_proba, preprocessed_corpus"
      ],
      "metadata": {
        "id": "BsFcsBfuDrBZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ngram('I am happy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChQpdNnaFGN3",
        "outputId": "25b48d7b-d717-4ddd-9a0f-d46748cbfa24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-15.368771915427438, '<s> <s> i am <UNK> </s>')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The log probability of -15.36 indicates that the sequence is relatively unlikely due to the presence of the unknown token."
      ],
      "metadata": {
        "id": "2FX9kkRxFzqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method: `test_perplexity(test_file)`\n",
        "\n",
        "This method takes the path of a testing corpus and calculates the perplexity based on this test corpus.\n"
      ],
      "metadata": {
        "id": "crtirHbzpcpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_perplexity(test_file):\n",
        "        log_proba, preprocessed_corpus = predict_ngram(test_file)\n",
        "        test_count_tokens = len(preprocessed_corpus.split(\" \"))\n",
        "        return math.exp(-log_proba / test_count_tokens) if test_count_tokens > 0 else float('inf')"
      ],
      "metadata": {
        "id": "tHUNABCLGUu2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity('ngramv1.test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtM4whdSGYtj",
        "outputId": "969cc708-6daf-4b94-d89e-56c46fa752b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.33709766718842"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The perplexity of approximately 10.33 indicates that the model has a reasonable understanding of the sequence.\n"
      ],
      "metadata": {
        "id": "ngIO1nqXp4fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will use a larger dataset to train our model, which will include three additional functionalities:\n",
        "\n",
        "### Method: `generateText`\n",
        "\n",
        "This method generates a sentence using an n-gram model, starting from the `<s>` tokens and sampling the next word based on its precomputed probability in `self.ngram_probabilities`.\n",
        "\n",
        "The process continues until the end token `</s>` is generated or the `max_length` is reached.\n",
        "\n",
        "### Method: `autoComplete`\n",
        "\n",
        "This method predicts the most probable next word based on the input text using n-gram probabilities.\n",
        "\n",
        "### Method: `correction`\n",
        "\n",
        "This method implements a variant of a spell checker using an n-gram language model to rank possible corrections based on their probabilities from the language model and their minimum edit distance. It corrects misspelled words in the input sentence based on known vocabulary and n-gram probabilities.\n"
      ],
      "metadata": {
        "id": "z9JU5m-tIBAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions\n",
        "To accomplish this, we will utilize the following functions. For a better understanding, please refer to the Spelling Corrector notebook available in the same repository."
      ],
      "metadata": {
        "id": "kfFwvKE-LHtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edits1(s):\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    # Split the string into all possible pairs of prefixes and suffixes\n",
        "    splits = [(s[:i], s[i:]) for i in range(len(s) + 1)]\n",
        "\n",
        "    # Generate words after deleting each letter\n",
        "    deletes = [a + b[1:] for a, b in splits if b]\n",
        "\n",
        "    # Generate words after inserting each letter of the alphabet at every position\n",
        "    inserts = [a + c + b for a, b in splits for c in alphabet]\n",
        "\n",
        "    # Generate words after replacing each letter with every letter of the alphabet\n",
        "    replaces = [a + c + b[1:] for a, b in splits if b for c in alphabet]\n",
        "\n",
        "    # Generate words after transposing two letters in the word\n",
        "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b) > 1]\n",
        "\n",
        "    # Return a set of unique words\n",
        "    return set(deletes + inserts + replaces + transposes)\n",
        "\n",
        "def edits2(word):\n",
        "    # Return the set of all words that are two edits away from the given word\n",
        "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
        "\n",
        "def knownWord(words, vocabulary):\n",
        "    # Get the set of valid words from the dictionary\n",
        "    return set(w for w in words if w in vocabulary)"
      ],
      "metadata": {
        "id": "JSwH8G__HBuR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NgramLanguageModel:\n",
        "\n",
        "    def __init__(self, ngram_size=2, count_threshold=2):\n",
        "        self.ngram_size = ngram_size\n",
        "        self.count_threshold = count_threshold\n",
        "        self.n_gram_counts = {}\n",
        "        self.nminus1_gram_counts = {}\n",
        "        self.vocabular = {}\n",
        "        self.k = 0.01\n",
        "        self.ngram_probabilities = {}\n",
        "\n",
        "    def prepare_data(self, infile, operation):\n",
        "        \"\"\"Prepare and preprocess the input data.\"\"\"\n",
        "        corpus = \"\"\n",
        "        pattern = r'^(\\/)?([^\\/\\0]+(\\/)?)+\\.[^\\/\\0]+$'\n",
        "\n",
        "        # Input handling: check if infile is a file or raw text\n",
        "        if operation or re.match(pattern, infile):\n",
        "            with open(infile, 'r', encoding=\"utf8\") as file:\n",
        "                corpus = file.read()\n",
        "        else:\n",
        "            corpus = infile\n",
        "\n",
        "        newtokens = []\n",
        "        token_counts = {}\n",
        "        unk_words = []\n",
        "\n",
        "        # Preprocess the entire corpus\n",
        "        corpus = corpus.lower()\n",
        "        corpus = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', corpus, flags=re.MULTILINE)  # Remove URLs\n",
        "\n",
        "        # Tokenize the corpus into sentences\n",
        "        sentences = sent_tokenize(corpus)\n",
        "\n",
        "        # Process each sentence\n",
        "        for sentence in sentences:\n",
        "            # Add start tokens for N-gram modeling\n",
        "            newtokens.extend(['<s>'] * (self.ngram_size - 1))\n",
        "\n",
        "            # Tokenize sentence into words\n",
        "            words = word_tokenize(sentence)\n",
        "\n",
        "            # Handle date tokens separately\n",
        "            for token in words:\n",
        "                dates = re.findall(r'(.*)((?:0?[1-9]|[12][0-9]|3[01])/(?:0?[1-9]|1[0-2])/(?:\\d{2}|\\d{4}))(.*)', token)\n",
        "                if dates:\n",
        "                    for items in dates:\n",
        "                        for date in items:\n",
        "                            if re.match(r\"(?:0?[1-9]|[12][0-9]|3[01])/(?:0?[1-9]|1[0-2])/(?:\\d{2}|\\d{4})\", date):\n",
        "                                newtokens.append(date)\n",
        "                                token_counts[date] = token_counts.get(date, 0) + 1\n",
        "                            else:\n",
        "                                date_tokens = word_tokenize(date)\n",
        "                                newtokens.extend(date_tokens)\n",
        "                                for x in date_tokens:\n",
        "                                    token_counts[x] = token_counts.get(x, 0) + 1\n",
        "                else:\n",
        "                    newtokens.append(token)\n",
        "                    token_counts[token] = token_counts.get(token, 0) + 1\n",
        "\n",
        "            # Add end-of-sentence token\n",
        "            newtokens.append('</s>')\n",
        "\n",
        "        # Handle unknown tokens\n",
        "        if operation:\n",
        "            for i, token in enumerate(newtokens):\n",
        "                if token_counts.get(token, 0) < self.count_threshold and token not in ['<s>', '</s>']:\n",
        "                    unk_words.append(token)\n",
        "                    token_counts['<UNK>'] = token_counts.get('<UNK>', 0) + 1\n",
        "                    newtokens[i] = '<UNK>'\n",
        "\n",
        "            self.vocabular = token_counts\n",
        "            return ' '.join(newtokens)\n",
        "        else:\n",
        "            for i, token in enumerate(newtokens):\n",
        "                if token not in self.vocabular and token not in ['<s>', '</s>']:\n",
        "                    unk_words.append(token)\n",
        "                    newtokens[i] = '<UNK>'\n",
        "\n",
        "            return ' '.join(newtokens), unk_words\n",
        "\n",
        "    def train(self, infile=None):\n",
        "        \"\"\"\n",
        "        Train the n-gram model using the prepared data.\n",
        "        Applies smoothing, handles OOV words, and calculates logarithmic probabilities.\n",
        "        \"\"\"\n",
        "        # Preprocess the input corpus\n",
        "        corpus = self.prepare_data(infile, True)\n",
        "\n",
        "        # Extract sentences from the preprocessed corpus\n",
        "        sentences = re.findall(r\"<s>.+?</s>\", corpus)\n",
        "\n",
        "        # Step 1: Count n-grams and (n-1)-grams\n",
        "        for captured_sentence in sentences:\n",
        "            tokens = captured_sentence.split(\" \")\n",
        "\n",
        "            for i in range(len(tokens) - (self.ngram_size - 1)):\n",
        "                # Extract n-gram and n-1-gram\n",
        "                n_gram = tokens[i:i + self.ngram_size]\n",
        "                n_gram_tuple = tuple(n_gram)\n",
        "                nminus1_gram_tuple = tuple(n_gram[:-1])\n",
        "\n",
        "                # Update counts\n",
        "                self.n_gram_counts[n_gram_tuple] = self.n_gram_counts.get(n_gram_tuple, 0) + 1\n",
        "                self.nminus1_gram_counts[nminus1_gram_tuple] = self.nminus1_gram_counts.get(nminus1_gram_tuple, 0) + 1\n",
        "\n",
        "        # Step 2: Add-k smoothing\n",
        "        vocab_size = len(self.vocabular)\n",
        "        ngram_probabilities = {}\n",
        "\n",
        "        for ngram, count in self.n_gram_counts.items():\n",
        "            n_minus1_gram = ngram[:-1]\n",
        "            count_nminus1 = self.nminus1_gram_counts.get(n_minus1_gram, 0)\n",
        "\n",
        "            # Add-k smoothing: P(wi | wi-1, ..., wi-(n-1)) = (count(wi-1,...,wi) + k) / (count(wi-1,...,wi-(n-1)) + k * V)\n",
        "            smoothed_prob = (count + self.k) / (count_nminus1 + self.k * vocab_size)\n",
        "\n",
        "            # Logarithmic probabilities\n",
        "            ngram_probabilities[ngram] = math.log(smoothed_prob)\n",
        "\n",
        "        # Save the probabilities for the model\n",
        "        self.ngram_probabilities = ngram_probabilities\n",
        "\n",
        "    def predict_ngram(self, sentence):\n",
        "      \"\"\"\n",
        "      Predict the log probability of a given sentence based on n-gram counts.\n",
        "\n",
        "      \"\"\"\n",
        "      log_proba = 0\n",
        "      preprocessed_corpus, _ = self.prepare_data(sentence, False)\n",
        "      sentences = re.findall(r\"<s>.+?</s>\", preprocessed_corpus)\n",
        "\n",
        "      for captured_sentence in sentences:\n",
        "          tokens = captured_sentence.split(\" \")\n",
        "\n",
        "          # Calculate the log probability for each n-gram in the sentence\n",
        "          for i in range(len(tokens) - (self.ngram_size - 1)):\n",
        "              n_gram = tokens[i:i + self.ngram_size]\n",
        "              n_gram_tuple = tuple(n_gram)\n",
        "              nminus1_gram_tuple = tuple(n_gram[:-1])\n",
        "\n",
        "              # Use add-k smoothing to calculate log probability\n",
        "              count_n_gram = self.n_gram_counts.get(n_gram_tuple, 0)\n",
        "              count_nminus1_gram = self.nminus1_gram_counts.get(nminus1_gram_tuple, 0)\n",
        "\n",
        "              smoothed_prob = (count_n_gram + self.k) / (count_nminus1_gram + self.k * len(self.vocabular))\n",
        "              log_proba += math.log(smoothed_prob)\n",
        "\n",
        "      return log_proba, preprocessed_corpus\n",
        "\n",
        "    def test_perplexity(self, test_file):\n",
        "        \"\"\"Calculate the perplexity of the test file.\"\"\"\n",
        "        log_proba, preprocessed_corpus = self.predict_ngram(test_file)\n",
        "        test_count_tokens = len(preprocessed_corpus.split(\" \"))\n",
        "        return math.exp(-log_proba / test_count_tokens) if test_count_tokens > 0 else float('inf')\n",
        "\n",
        "    def generate_text(self, max_length=20):\n",
        "      \"\"\"\n",
        "      Generate a sentence using an n-gram model, starting from the <s> tokens\n",
        "      and sampling the next word based on its precomputed probability in self.ngram_probabilities.\n",
        "\n",
        "      The process continues until the end token </s> is generated or the max_length is reached.\n",
        "\n",
        "      Parameters:\n",
        "      - max_length: The maximum length of the generated sentence to avoid infinite loops.\n",
        "\n",
        "      Returns:\n",
        "      - generated_sentence: The generated sentence as a string.\n",
        "      \"\"\"\n",
        "      # Initialize with the start token sequence <s>, based on ngram_size-1\n",
        "      current_ngram = tuple([\"<s>\"] * (self.ngram_size - 1))\n",
        "      generated_sentence = list(current_ngram)  # List to accumulate generated words\n",
        "\n",
        "      while current_ngram[-1] != \"</s>\" and len(generated_sentence) < max_length:\n",
        "          # Collect all n-grams that start with the current ngram (n-1 words)\n",
        "          next_word_candidates = {ngram[-1]: prob for ngram, prob in self.ngram_probabilities.items()\n",
        "                                  if ngram[:-1] == current_ngram}\n",
        "\n",
        "          if not next_word_candidates:\n",
        "              # If no valid candidates are found (which can happen with unknown tokens), break the loop\n",
        "              break\n",
        "\n",
        "          # Normalize probabilities for the next word\n",
        "          total_prob = sum(math.exp(prob) for prob in next_word_candidates.values())\n",
        "          next_word_probabilities = {word: math.exp(prob) / total_prob for word, prob in next_word_candidates.items()}\n",
        "\n",
        "          # Sample the next word based on the precomputed probabilities\n",
        "          next_word = np.random.choice(list(next_word_probabilities.keys()), p=list(next_word_probabilities.values()))\n",
        "\n",
        "          # Ensure that the sampled word is not <UNK>\n",
        "          while next_word == \"<UNK>\":\n",
        "              next_word = np.random.choice(list(next_word_probabilities.keys()), p=list(next_word_probabilities.values()))\n",
        "\n",
        "          # Add the next word to the generated sentence\n",
        "          generated_sentence.append(next_word)\n",
        "\n",
        "          # Update the current ngram by shifting it to include the new word\n",
        "          current_ngram = tuple(generated_sentence[-(self.ngram_size - 1):])\n",
        "\n",
        "      # Join the generated words into a sentence and return, excluding <s> and </s> tokens\n",
        "      return ' '.join(generated_sentence[self.ngram_size-1:-1])\n",
        "\n",
        "    def auto_complete(self, sentence):\n",
        "      \"\"\"\n",
        "      Predict the most probable next word based on the input text using n-gram probabilities.\n",
        "\n",
        "      Parameters:\n",
        "      - sentence: The input sentence for which the next word needs to be predicted.\n",
        "\n",
        "      Returns:\n",
        "      - complete_sentence: The input sentence followed by the most probable next word.\n",
        "      \"\"\"\n",
        "      if not sentence:\n",
        "          return \"<UNK>\"\n",
        "\n",
        "      # Preprocess the input text\n",
        "      preprocessed_sentence, _ = self.prepare_data(sentence, False)\n",
        "\n",
        "      tokens = preprocessed_sentence.split(\" \")\n",
        "      filtered_tokens = [token for token in tokens if token != \"</s>\"]\n",
        "\n",
        "      # Get the last n-1 tokens for n-gram prediction\n",
        "      current_ngram = filtered_tokens[-(self.ngram_size - 1):]\n",
        "\n",
        "      # Pad with <s> if not enough tokens are available\n",
        "      if len(current_ngram) < self.ngram_size - 1:\n",
        "          current_ngram = [\"<s>\"] * ((self.ngram_size - 1) - len(current_ngram)) + current_ngram\n",
        "\n",
        "      next_word = \"\"\n",
        "\n",
        "      # Collect all n-grams that start with the current n-1 tokens\n",
        "      next_word_candidates = {ngram[-1]: prob for ngram, prob in self.ngram_probabilities.items()\n",
        "                              if ngram[:-1] == tuple(current_ngram)}\n",
        "\n",
        "      # Normalize probabilities for the next word\n",
        "      total_prob = sum(math.exp(prob) for prob in next_word_candidates.values())\n",
        "      next_word_probabilities = {word: math.exp(prob) / total_prob for word, prob in next_word_candidates.items()}\n",
        "\n",
        "      # Find the next word with the maximum probability\n",
        "      next_word = max(next_word_probabilities, key=next_word_probabilities.get)\n",
        "\n",
        "      # Return the complete sentence with the predicted next word\n",
        "      if next_word:\n",
        "          return sentence.strip() + \" \" + next_word\n",
        "\n",
        "      return \"<UNK>\"\n",
        "\n",
        "    def correction(self, sentence):\n",
        "      \"\"\"\n",
        "      Corrects misspelled words in the input sentence based on known vocabulary and n-gram probabilities.\n",
        "\n",
        "      Parameters:\n",
        "          sentence (str): The input sentence with potential misspelled words.\n",
        "\n",
        "      Returns:\n",
        "          str: The corrected sentence with the most probable word substitutions based on n-gram probabilities.\n",
        "      \"\"\"\n",
        "      # Prepare the data (preprocess and get unknown words)\n",
        "      preprocessed_sentence, unk_words = self.prepare_data(sentence, False)\n",
        "\n",
        "      # Dictionary to store the corrections for unknown words\n",
        "      corrections = {}\n",
        "\n",
        "      # Iterate through each unknown word\n",
        "      for unk in unk_words:\n",
        "          # List to store candidates for correction\n",
        "          candidate_list = []\n",
        "\n",
        "          # Get known words with an edit distance of 1\n",
        "          e1_words = edits1(unk)\n",
        "          valid_e1 = knownWord(e1_words, self.vocabular)\n",
        "          if valid_e1:\n",
        "              candidate_list.extend(valid_e1)\n",
        "\n",
        "          # Get known words with an edit distance of 2\n",
        "          e2_words = edits2(unk)\n",
        "          valid_e2 = knownWord(e2_words, self.vocabular)\n",
        "          if valid_e2:\n",
        "              candidate_list.extend(valid_e2)\n",
        "\n",
        "          # Add the original unknown word to the candidate list\n",
        "          candidate_list.append(unk)\n",
        "\n",
        "          # Choose the most probable candidate based on n-gram probabilities\n",
        "          corrections[unk] = max(candidate_list, key=lambda x: self.ngram_probabilities.get((x,), 0))\n",
        "\n",
        "      # Tokenize the preprocessed sentence\n",
        "      tokens = preprocessed_sentence.split(\" \")\n",
        "\n",
        "      # Replace each \"<UNK>\" with its correction\n",
        "      corrected_tokens = []\n",
        "      unk_index = 0\n",
        "      for token in tokens:\n",
        "          if token == \"<UNK>\":\n",
        "              corrected_tokens.append(corrections[unk_words[unk_index]])\n",
        "              unk_index += 1\n",
        "          else:\n",
        "              corrected_tokens.append(token)\n",
        "\n",
        "      # Join the corrected tokens back into a sentence\n",
        "      return ' '.join(corrected_tokens[self.ngram_size-1:-1])\n"
      ],
      "metadata": {
        "id": "e_UaX-ztUiOb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram = NgramLanguageModel()"
      ],
      "metadata": {
        "id": "T2vqGBO5BCYt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram.train('data.txt')"
      ],
      "metadata": {
        "id": "TFOIRk0eBGb3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram.generate_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EcvF3YO9R8v8",
        "outputId": "8b1b73b8-5b34-4b7d-cede-8d6853f64e69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"is in anaheim , it 's kind of music , but wondering if so bad twitter network .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram.auto_complete(\"happy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j7CM1GKewws0",
        "outputId": "5517f6df-a439-41dc-a5c9-75b237959f1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy birthday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram.correction(\"happy birthda brather\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jkVp6DXcieGH",
        "outputId": "d52cd538-6fdf-4487-8f27-48e9fd40eeb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy birthday brother'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}