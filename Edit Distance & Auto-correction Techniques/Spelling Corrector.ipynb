{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Corrector"
      ],
      "metadata": {
        "id": "5fuU0I0IX5ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider the problem of determining the most probable correction for a word not found in the dictionary. Thus, the problem is to find the correction `c`, among all possible candidate corrections, that maximizes the probability that `c` is the intended correction, given the original word `w`:\n",
        "\n",
        "$$ \\text{argmax}_{c \\in \\text{candidates}} P(c|w) $$\n",
        "\n",
        "By Bayes' theorem, this is equivalent to:\n",
        "\n",
        "$$ \\text{argmax}_{c \\in \\text{candidates}} \\frac{P(c) P(w|c)}{P(w)} $$\n",
        "\n",
        "Since \\( P(w) \\) is the same for each possible candidate `c`, we can eliminate it, which gives:\n",
        "\n",
        "$$ \\text{argmax}_{c \\in \\text{candidates}} P(c) P(w|c) $$\n",
        "\n",
        "The elements of this equation are:\n",
        "\n",
        "- **Language Model: \\( P(c) \\)** - The probability that `c` appears as a word in an English text.\n",
        "- **Error Model: \\( P(w|c) \\)** - The probability that `w` was typed when the author intended to write `c`.\n"
      ],
      "metadata": {
        "id": "332sGR1AURy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to solve this problem, we will start by implementing the function `process_data(corpus_file)` which reads the given corpus from a text file, converts the text to lowercase, segments the text, and returns a list of words."
      ],
      "metadata": {
        "id": "Fw2qYln5VmKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the nltk library for corpus preprocessing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Downloading the pre-trained 'punkt' tokenizer model for word segmentation\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOs1qDyRUzbh",
        "outputId": "63d0b7a5-a5de-428c-f224-455f6625dc99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(corpus_file):\n",
        "    \"\"\"\n",
        "    Process the input corpus file, segmenting it into words, converting to lowercase,\n",
        "    and filtering out non-alphanumeric tokens.\n",
        "\n",
        "    Parameters:\n",
        "    corpus_file (str): Path to the text file containing the corpus.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of cleaned, lowercase words from the corpus.\n",
        "    \"\"\"\n",
        "    # List to store the processed words\n",
        "    words = []\n",
        "\n",
        "    # Reading the corpus from the file\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenizing the text using NLTK's word tokenizer\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Processing each token\n",
        "    for token in tokens:\n",
        "        # Filtering out non-alphanumeric tokens\n",
        "        if token.isalnum():\n",
        "            # Adding the token in lowercase to the list\n",
        "            words.append(token.lower())\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "rOctl3acWK7K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will implement the function `get_vocabulary(corpus_file)` that returns the vocabulary built from a corpus passed as an argument to the function."
      ],
      "metadata": {
        "id": "w6TMzXywZak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(corpus_file):\n",
        "    \"\"\"\n",
        "    Extracts the vocabulary from a given text corpus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus_file : str\n",
        "        The path to the text file containing the corpus.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "        A set of unique words in lowercase, containing only alphanumeric tokens.\n",
        "    \"\"\"\n",
        "    # Initialize a set to store unique words\n",
        "    vocabulary = set()\n",
        "\n",
        "    # Read the corpus\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenize the text using NLTK\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    for token in tokens:\n",
        "        # Remove tokens that are not alphanumeric\n",
        "        if token.isalnum():\n",
        "            # Add the token in lowercase to the vocabulary set\n",
        "            vocabulary.add(token.lower())\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "FU7mMRPBZbOq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can estimate the probability of a word by counting the number of times that word appears in a large text corpus and dividing by the size of the corpus. We will write a function that constructs the language model by calculating the probability of each word based on the provided text file (I will use: The Project Gutenberg EBook of The Adventures of Sherlock Holmes by Arthur Conan Doyle) and store the results in an appropriate data structure.\n",
        "\n",
        "You can get the file from here: [The Adventures of Sherlock Holmes](https://www.gutenberg.org/ebooks/1661)"
      ],
      "metadata": {
        "id": "xX7PG5Uo508i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_language_model(corpus_file):\n",
        "    \"\"\"\n",
        "    Constructs a language model by calculating the probability of each word in the provided corpus.\n",
        "\n",
        "    The probability of a word is estimated by counting its occurrences in the text and dividing\n",
        "    by the total number of words in the corpus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus_file : str\n",
        "        The path to the text file containing the corpus.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary where the keys are words and the values are their respective probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pre-process the corpus\n",
        "    words = process_data(corpus_file)\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = {}\n",
        "    total_words = len(words)\n",
        "    for word in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1  # Increment the count correctly\n",
        "\n",
        "    # Calculate probabilities\n",
        "    language_model = {}\n",
        "    for word, count in word_counts.items():\n",
        "        language_model[word] = count / total_words\n",
        "\n",
        "    return language_model"
      ],
      "metadata": {
        "id": "GQn_C2Kl518L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function Definitions\n",
        "\n",
        "1. **edits1(s)**:\n",
        "   - This function returns the set of all strings (whether they are words or not) that can be obtained by making a single modification (insertion, substitution, or deletion) to the string `s`.\n",
        "\n",
        "2. **edits2(s)**:\n",
        "   - This function returns the set of all strings (whether they are words or not) that can be obtained by making two modifications (insertion, substitution, or deletion) to the string `s`.\n",
        "\n",
        "3. **knownWord(words)**:\n",
        "   - This function filters the words in the list `words` that are not in the dictionary, retaining only the valid words. You can use the `get_vocabulary(corpus_file)` function to obtain the list of valid words from the dictionary."
      ],
      "metadata": {
        "id": "INOujka_Jrlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edits1(s):\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    # Split the string into all possible pairs of prefixes and suffixes\n",
        "    splits = [(s[:i], s[i:]) for i in range(len(s) + 1)]\n",
        "\n",
        "    # Generate words after deleting each letter\n",
        "    deletes = [a + b[1:] for a, b in splits if b]\n",
        "\n",
        "    # Generate words after inserting each letter of the alphabet at every position\n",
        "    inserts = [a + c + b for a, b in splits for c in alphabet]\n",
        "\n",
        "    # Generate words after replacing each letter with every letter of the alphabet\n",
        "    replaces = [a + c + b[1:] for a, b in splits if b for c in alphabet]\n",
        "\n",
        "    # Generate words after transposing two letters in the word\n",
        "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b) > 1]\n",
        "\n",
        "    # Return a set of unique words\n",
        "    return set(deletes + inserts + replaces + transposes)"
      ],
      "metadata": {
        "id": "AjzeYgiDJtgU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edits2(word):\n",
        "    # Return the set of all words that are two edits away from the given word\n",
        "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
      ],
      "metadata": {
        "id": "3q3SxspiJ4mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knownWord(words, corpus_file):\n",
        "    # Get the set of valid words from the dictionary\n",
        "    vocabulary = get_vocabulary(corpus_file)\n",
        "    return set(w for w in words if w in vocabulary)"
      ],
      "metadata": {
        "id": "tH0HZ61uKdKK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that we do not have data to construct the error model; therefore, we will adopt the following assumptions: all known words with an edit distance of 1 are infinitely more probable than known words with an edit distance of 2, and infinitely less probable than a known word with an edit distance of 0. Thus, to select the most probable candidates, we consider their probabilities based on the previously constructed language model and their priorities according to their edit distance from the original word. With this simplification, we do not need to multiply by a factor \\( P(w|c) \\), since each candidate with the chosen priority will have the same probability.\n",
        "\n",
        "We will create the function `candidates(word, corpus_file)` that returns the first non-empty list of candidates in order of priority:\n",
        "- The original word, if it is known; otherwise,\n",
        "- The list of known words at an edit distance of one, if there are any; otherwise,\n",
        "- The list of known words at an edit distance of two, if there are any; otherwise,\n",
        "- The original word, even if it is not known."
      ],
      "metadata": {
        "id": "NjVhqhNlMQqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def candidates(word , corpus_file):\n",
        "    \"\"\"\n",
        "    Generates a list of probable candidates for the given word based on edit distance.\n",
        "\n",
        "    The function checks if the original word is known, and if not, it looks for words\n",
        "    with an edit distance of 1 or 2 that are in the vocabulary obtained from the corpus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "        The word for which to find candidates.\n",
        "    corpus_file : str\n",
        "        The path to the text file used as the corpus to build the vocabulary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of candidate words in order of priority:\n",
        "        - The original word if it is known.\n",
        "        - Words known with an edit distance of 1.\n",
        "        - Words known with an edit distance of 2.\n",
        "        - The original word, even if it is not known.\n",
        "    \"\"\"\n",
        "    # List to store candidates\n",
        "    candidate_list = []\n",
        "    vocabulary = get_vocabulary(corpus_file)\n",
        "\n",
        "    # Check if the original word is known\n",
        "    if word in vocabulary:\n",
        "        candidate_list.append(word)\n",
        "\n",
        "    # Get known words with an edit distance of 1\n",
        "    e1_words = edits1(word)\n",
        "    valid_e1 = knownWord(e1_words, corpus_file)\n",
        "    if valid_e1:\n",
        "        candidate_list.extend(valid_e1)\n",
        "\n",
        "    # Get known words with an edit distance of 2\n",
        "    e2_words = edits2(word)\n",
        "    valid_e2 = knownWord(e2_words, corpus_file)\n",
        "    if valid_e2:\n",
        "        candidate_list.extend(valid_e2)\n",
        "\n",
        "    # Add the original word to the candidate list\n",
        "    candidate_list.append(word)\n",
        "\n",
        "    # Return the first non-empty list of candidates\n",
        "    return candidate_list"
      ],
      "metadata": {
        "id": "cpFFKmP_MTpO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the previous functions, we will write the function `correction(word, k, corpus_file)` which returns the `k` most probable corrections for the word `word`."
      ],
      "metadata": {
        "id": "tbamWzWjNwSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correction(word, k, corpus_file):\n",
        "    \"\"\"\n",
        "    Returns the k most probable corrections for the given word.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "        The word to correct.\n",
        "    k : int\n",
        "        The number of top corrections to return.\n",
        "    corpus_file : str\n",
        "        The path to the corpus file used for building the language model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of str\n",
        "        A list of the k most probable corrections for the word.\n",
        "    \"\"\"\n",
        "    # Get candidates for the given word\n",
        "    candidates_list = candidates(word, corpus_file)\n",
        "\n",
        "    # Calculate scores for each candidate using the language model\n",
        "    language_model = build_language_model(corpus_file)\n",
        "    candidate_scores = {candidate: language_model.get(candidate, 0) for candidate in candidates_list}\n",
        "\n",
        "    # Sort candidates by descending score\n",
        "    sorted_candidates = sorted(candidate_scores, key=candidate_scores.get, reverse=True)\n",
        "\n",
        "    # Return the top k candidates\n",
        "    return sorted_candidates[:k]"
      ],
      "metadata": {
        "id": "TJNEV9glPENK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "corrections = correction('heloo', 5, 'corpus.txt')\n",
        "print(corrections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pj3VmEGNxDb",
        "outputId": "bc9e2ab4-a855-402c-eec1-8583ed589f01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['held', 'help', 'below', 'hero', 'helen']\n"
          ]
        }
      ]
    }
  ]
}